{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"colab":{"provenance":[],"collapsed_sections":["ACPLe_gVTWWE"],"gpuType":"T4"},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"6f75872145ad4562bff48a1485d7f925":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_865fc57f8b314285a46ab58d4ade1547","IPY_MODEL_8ec3576cabbb4248ba70cc9095d1922e","IPY_MODEL_28a1034ea9d649d2853e3adb827b4fd7"],"layout":"IPY_MODEL_44414d3190004dc9b5cf19876440984e"}},"865fc57f8b314285a46ab58d4ade1547":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_453f0a61995b4110b186d9b5c3aa969f","placeholder":"​","style":"IPY_MODEL_01b8e02ee2ae497699dae1e8029e8d17","value":"README.md: 100%"}},"8ec3576cabbb4248ba70cc9095d1922e":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_dec91fbcfc4140bd9fa238b0ea98acaf","max":6200,"min":0,"orientation":"horizontal","style":"IPY_MODEL_7fd37ac1bbc44c34bcadbf256289c7e9","value":6200}},"28a1034ea9d649d2853e3adb827b4fd7":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_0c8a303c4ede4e93830acdba33eedf1e","placeholder":"​","style":"IPY_MODEL_cf096e06764e4d92979932e0f63bc5c3","value":" 6.20k/6.20k [00:00&lt;00:00, 679kB/s]"}},"44414d3190004dc9b5cf19876440984e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"453f0a61995b4110b186d9b5c3aa969f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"01b8e02ee2ae497699dae1e8029e8d17":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"dec91fbcfc4140bd9fa238b0ea98acaf":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7fd37ac1bbc44c34bcadbf256289c7e9":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"0c8a303c4ede4e93830acdba33eedf1e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"cf096e06764e4d92979932e0f63bc5c3":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"6c1cac9638e543d0b3e7df00ec901b62":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_9afd237e0a794b1eb6065ad404c7f85d","IPY_MODEL_23b79a8d44d746cbb89b2137a53b0a0e","IPY_MODEL_c0c0a39492624877b40d4d84b2d3c8dc"],"layout":"IPY_MODEL_f83efd297d0c4b599761c034fcea9005"}},"9afd237e0a794b1eb6065ad404c7f85d":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_ae0269a64ee447e69cda46ef0d328458","placeholder":"​","style":"IPY_MODEL_55dd5765ad584f5f934162899a09d723","value":"Assignment-3/naive_bayes/train_nb.parque(…): 100%"}},"23b79a8d44d746cbb89b2137a53b0a0e":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_4756a4fc100c46ad872b4cfec8311e59","max":190331496,"min":0,"orientation":"horizontal","style":"IPY_MODEL_3cd65d317e5b4ac4a30497a5ed670b4f","value":190331496}},"c0c0a39492624877b40d4d84b2d3c8dc":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_fc3ab1abf07e4306b0616a76a78705f7","placeholder":"​","style":"IPY_MODEL_4a1e5076decb4f578379028451510019","value":" 190M/190M [00:02&lt;00:00, 79.5MB/s]"}},"f83efd297d0c4b599761c034fcea9005":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ae0269a64ee447e69cda46ef0d328458":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"55dd5765ad584f5f934162899a09d723":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"4756a4fc100c46ad872b4cfec8311e59":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3cd65d317e5b4ac4a30497a5ed670b4f":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"fc3ab1abf07e4306b0616a76a78705f7":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4a1e5076decb4f578379028451510019":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"a1a345bb8d69413498f0d0939141df8c":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_4b45f1aa9e7c4122ad309009e8075c9e","IPY_MODEL_77824d7e339040ebaf32479dc2338867","IPY_MODEL_29620544aa2444ab87dcea808111529d"],"layout":"IPY_MODEL_e14aba46aed24438b70a3c364e32b4cf"}},"4b45f1aa9e7c4122ad309009e8075c9e":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_9b4d18448b124a29ad83d5988b034c5f","placeholder":"​","style":"IPY_MODEL_3494e749363e4eafa16cc5e329582137","value":"Assignment-3/naive_bayes/test_nb_with_la(…): 100%"}},"77824d7e339040ebaf32479dc2338867":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_d4332adb6b954761af76f743cf6d0f89","max":47721734,"min":0,"orientation":"horizontal","style":"IPY_MODEL_226586fbe4f3488f8ab61570943d40c1","value":47721734}},"29620544aa2444ab87dcea808111529d":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_d92170facf124139b4f6d634379f8f18","placeholder":"​","style":"IPY_MODEL_e24603c37f5a4c9f99a5f216ae3fb115","value":" 47.7M/47.7M [00:00&lt;00:00, 80.0MB/s]"}},"e14aba46aed24438b70a3c364e32b4cf":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9b4d18448b124a29ad83d5988b034c5f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3494e749363e4eafa16cc5e329582137":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"d4332adb6b954761af76f743cf6d0f89":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"226586fbe4f3488f8ab61570943d40c1":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"d92170facf124139b4f6d634379f8f18":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e24603c37f5a4c9f99a5f216ae3fb115":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"26dc99dabd1444989e4c85dee6f74dfb":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_178119ea55364da18d6fc77e7799fc70","IPY_MODEL_b17e10eb1e084ced9e50a3dd594c14b9","IPY_MODEL_10c6f20ba4f54fbeb98c3ab9fe25fe90"],"layout":"IPY_MODEL_e066f5f98afa4eacba78adee72f7f7cd"}},"178119ea55364da18d6fc77e7799fc70":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_0e58263878bd4489b7fc4f342739f4a5","placeholder":"​","style":"IPY_MODEL_dcc0db14069a47f8953f0f53f1912ccb","value":"Generating train split: 100%"}},"b17e10eb1e084ced9e50a3dd594c14b9":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_cdd2091ce65e4628b9844af78ebd9314","max":8000,"min":0,"orientation":"horizontal","style":"IPY_MODEL_4444efb2c2c2449a8ecb458483867e5f","value":8000}},"10c6f20ba4f54fbeb98c3ab9fe25fe90":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_f8da7f3a21ac4805bc93395978d1b4d6","placeholder":"​","style":"IPY_MODEL_0770b0113bc747a1b50fca736c875412","value":" 8000/8000 [00:01&lt;00:00, 6108.77 examples/s]"}},"e066f5f98afa4eacba78adee72f7f7cd":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0e58263878bd4489b7fc4f342739f4a5":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"dcc0db14069a47f8953f0f53f1912ccb":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"cdd2091ce65e4628b9844af78ebd9314":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4444efb2c2c2449a8ecb458483867e5f":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"f8da7f3a21ac4805bc93395978d1b4d6":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0770b0113bc747a1b50fca736c875412":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"be0f0205a5714635acb82fab3bb92d9b":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_6dbd8dc4787243d7a7083384716ef544","IPY_MODEL_a7544e61b3614d9382eef932b569faa4","IPY_MODEL_8e023c2f5ac54c5ca64b0e1c2a46269b"],"layout":"IPY_MODEL_042671c5c4b346f7bbb0f2728af52092"}},"6dbd8dc4787243d7a7083384716ef544":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_e1b3a3bf825b41a1989221c06c58a14f","placeholder":"​","style":"IPY_MODEL_d8d91c3510b34f6eb1f6b65ca4eeb8a1","value":"Generating test split: 100%"}},"a7544e61b3614d9382eef932b569faa4":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_d7f991b028c34e2b92ca59659d128da9","max":2000,"min":0,"orientation":"horizontal","style":"IPY_MODEL_64c56b4d5c0a42c7b6646b0d2c562e67","value":2000}},"8e023c2f5ac54c5ca64b0e1c2a46269b":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_c4680f9751dc4b3ea1a10075dbdf6b89","placeholder":"​","style":"IPY_MODEL_a69b6758caa5496aad1071f25b139a06","value":" 2000/2000 [00:00&lt;00:00, 3086.60 examples/s]"}},"042671c5c4b346f7bbb0f2728af52092":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e1b3a3bf825b41a1989221c06c58a14f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d8d91c3510b34f6eb1f6b65ca4eeb8a1":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"d7f991b028c34e2b92ca59659d128da9":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"64c56b4d5c0a42c7b6646b0d2c562e67":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"c4680f9751dc4b3ea1a10075dbdf6b89":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a69b6758caa5496aad1071f25b139a06":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"3f02ffbc1fea4d059258d489fac45156":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_503d3c2ba7614c04b8834fd6443dedab","IPY_MODEL_84c4091127c049598f890d7107f26d2b","IPY_MODEL_44e77c7cd6734ca09cdd2f06466d9ce1"],"layout":"IPY_MODEL_336e136c126847208a25e3303d3b72fc"}},"503d3c2ba7614c04b8834fd6443dedab":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_ece73446c3c44707876fb94e9042bd88","placeholder":"​","style":"IPY_MODEL_7000c80d356e4e89a4423b1131249e53","value":"Assignment-3/em/train_em.parquet: 100%"}},"84c4091127c049598f890d7107f26d2b":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_4b307e6892564a44860b45e4805eddd8","max":188882611,"min":0,"orientation":"horizontal","style":"IPY_MODEL_3d5b5259b8284fbd9fd11b82cca8d851","value":188882611}},"44e77c7cd6734ca09cdd2f06466d9ce1":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_f2545a9975b44d7ba0bd5dce57701723","placeholder":"​","style":"IPY_MODEL_6e75e4b110504ec5b6d60558f9dc5c0a","value":" 189M/189M [00:02&lt;00:00, 124MB/s]"}},"336e136c126847208a25e3303d3b72fc":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ece73446c3c44707876fb94e9042bd88":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7000c80d356e4e89a4423b1131249e53":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"4b307e6892564a44860b45e4805eddd8":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3d5b5259b8284fbd9fd11b82cca8d851":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"f2545a9975b44d7ba0bd5dce57701723":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6e75e4b110504ec5b6d60558f9dc5c0a":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"6c788ad249e54c1c8aee767cd8b18997":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_15103d465d3e4520b9e72e92decf0f3b","IPY_MODEL_9890b4ca7ec74b7b9cd7fdce61fc1199","IPY_MODEL_97545fc5e85a40d2aacf12514e4bb49c"],"layout":"IPY_MODEL_76353109f1bf4719a5a68f1a15dbd3b4"}},"15103d465d3e4520b9e72e92decf0f3b":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_bc4e36d4b3d743ba82c2487364ba2722","placeholder":"​","style":"IPY_MODEL_b2927bf355944ecfb36a88ef2e98888b","value":"Assignment-3/em/test_em.parquet: 100%"}},"9890b4ca7ec74b7b9cd7fdce61fc1199":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_285582eb655b41258e9190633e9b8dbc","max":47700874,"min":0,"orientation":"horizontal","style":"IPY_MODEL_76d17503ee1e49a5b4c92b84b851ec88","value":47700874}},"97545fc5e85a40d2aacf12514e4bb49c":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_e2952b4ff343498aa55060cff0831410","placeholder":"​","style":"IPY_MODEL_94132eea8c5449fe8cd1d84cc19d9eaf","value":" 47.7M/47.7M [00:00&lt;00:00, 77.3MB/s]"}},"76353109f1bf4719a5a68f1a15dbd3b4":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"bc4e36d4b3d743ba82c2487364ba2722":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b2927bf355944ecfb36a88ef2e98888b":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"285582eb655b41258e9190633e9b8dbc":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"76d17503ee1e49a5b4c92b84b851ec88":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"e2952b4ff343498aa55060cff0831410":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"94132eea8c5449fe8cd1d84cc19d9eaf":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"2202c5fc82d14ef990e6fbb54379e47d":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_5309a82c82f7467abf2d889a01126029","IPY_MODEL_927c0a15a5c34d15958bcaf44eae9fc7","IPY_MODEL_0e556fc39e3546c6ae9f392de919e90b"],"layout":"IPY_MODEL_42f7a138f2ac40f5a769e765a6bfeddf"}},"5309a82c82f7467abf2d889a01126029":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_385a346930994317b62d49de6b095389","placeholder":"​","style":"IPY_MODEL_12b5e0bf07aa493c85731c9318144fe9","value":"Generating train split: 100%"}},"927c0a15a5c34d15958bcaf44eae9fc7":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_83bb1a1caa6f456baf0210197095ddfa","max":8000,"min":0,"orientation":"horizontal","style":"IPY_MODEL_d543594ff21348bc9113cc1be2ac4de1","value":8000}},"0e556fc39e3546c6ae9f392de919e90b":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_a2e1ccce089946b6a9f55f806bfb6147","placeholder":"​","style":"IPY_MODEL_62fc51f3f7b6450c960ebc28872a4918","value":" 8000/8000 [00:04&lt;00:00, 1046.59 examples/s]"}},"42f7a138f2ac40f5a769e765a6bfeddf":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"385a346930994317b62d49de6b095389":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"12b5e0bf07aa493c85731c9318144fe9":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"83bb1a1caa6f456baf0210197095ddfa":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d543594ff21348bc9113cc1be2ac4de1":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"a2e1ccce089946b6a9f55f806bfb6147":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"62fc51f3f7b6450c960ebc28872a4918":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"7bbe56f144a04dc4ac37f1a7aa548ab7":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_d236f809d21649f98fc9080eb7b71e99","IPY_MODEL_0291fd12809a4332bfce8a67c122e0b3","IPY_MODEL_7e4a9aa3a31f4b48840d54e9a3730996"],"layout":"IPY_MODEL_0ee685e086e84f73885e10f15b27619f"}},"d236f809d21649f98fc9080eb7b71e99":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_dec03067fa9048b1987d56c5fe94ed89","placeholder":"​","style":"IPY_MODEL_aef2530111744ab99a3dc989bcaa342c","value":"Generating test split: 100%"}},"0291fd12809a4332bfce8a67c122e0b3":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_e6275804141b4c78a164e063193cb8c9","max":2000,"min":0,"orientation":"horizontal","style":"IPY_MODEL_7313529f5dfe4bbb905f4b57a9556728","value":2000}},"7e4a9aa3a31f4b48840d54e9a3730996":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_ef2c5127140f4fbc959e6f6406330e49","placeholder":"​","style":"IPY_MODEL_51c185c0cfbc4a16924284fcefd77db1","value":" 2000/2000 [00:00&lt;00:00, 5030.49 examples/s]"}},"0ee685e086e84f73885e10f15b27619f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"dec03067fa9048b1987d56c5fe94ed89":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"aef2530111744ab99a3dc989bcaa342c":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"e6275804141b4c78a164e063193cb8c9":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7313529f5dfe4bbb905f4b57a9556728":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"ef2c5127140f4fbc959e6f6406330e49":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"51c185c0cfbc4a16924284fcefd77db1":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":594398,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":444885,"modelId":461369}],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# **Assignment 3**\n\nSubmission Form: https://forms.gle/udUXBCFjKXRUSzKD7\n\nDeadline: 30-Sep-2025, 11:59 PM","metadata":{"id":"PoXAzWIdaL5L"}},{"cell_type":"markdown","source":"# **Assignment Instructions:**\n\n1. Read all Instructions carefully provided for each question before beginning your work.\n\n2. Analyze each question thoroughly and document your result, and\nanalysis within the Google Colab notebook itself.\n\n\n3. This is an individual assignment. Your work should be original. Copying from peers or online sources is strictly prohibited.\n\n4. The use of AI tools like ChatGPT, Copilot, Gemini, LLMs or any other automated code generation tools for writing code is strictly forbidden.\n\n5. Clearly document your code with comments and explanations so that it is easy to understand your approach and thought process. It is ok to take help from some external tutorial; however cite it in your documenation otherwise it will be considered plagiarism.\n\n6. Follow the submission guidelines strictly. Make sure your notebook is well-organized and includes all necessary code, explanations, and outputs.\n\n7. The dataset usage instructions are present along with each task along with the README.md on [Exploration-Lab/CS779-Fall25](https://huggingface.co/datasets/Exploration-Lab/CS779-Fall25)\n\n8. **For the assignment submission you will have to implement all the tasks on this colab notebook with clear explanations for the complete code. Then, download this colab notebook as .ipynb file, zip it along with the expected deliverables mentioned for each task. Finally, submit the zip file via this form: https://forms.gle/udUXBCFjKXRUSzKD7** (Look at the [Final Submission Guidelines] below)\n9. The name of the zip file should follow this format: `CS779-A3-[Firstname]-[Lastname]-[Rollno].zip` (Just as in discord) where you have to replace [Firstname] with your actual first name and same for [Lastname] and [Rollno]. If you fail to do this, then we will not able to recover your assignment from pool of assignments as the process is automated.\n\n10. **The deadline for submission is September 30, 2025, 11:59 PM. Note that this is a strict deadline.**\n\n11. The above form will close at the above mentioned deadline and no further solutions will be accepted either by email or by any other means.\n\n12. If you have any doubt or get stuck in any problem, consult  TA's over Discord. It's better to take help of TAs than cheating.\n\n","metadata":{"id":"9hhaez6daJbI"}},{"cell_type":"markdown","source":"# Final Submission Guidelines (Auto-Evaluated Assignment)\n\nYour submission is **auto-evaluated**. Any deviation from these rules will result in **0 marks**. Read carefully and follow exactly.\n\n## What to submit (zip content)\n\nSubmit **one zip file** named: `CS779-A3-[Firstname]-[Lastname]-[Rollno].zip` (Just as in discord)\n\nExample: `CS779-A3-James-Bond-007.zip`\n\nThis zip must contain **the exact deliverables mentioned below following the exact folder structure along with a python notebook contaning your implementation code and answers to critical analysis questions at the root level** (no extra files, no data, no virtualenvs):\n\n```\nCS779-A3-[Firstname]-[Lastname]-[Rollno].zip\n├── CS779-A3-[Firstname]-[Lastname]-[Rollno].ipynb\n├── Task_1\n|   ├── skipgram_ns.pkl\n|   ├── skipgram_hs.pkl\n|   ├── cbow_ns.pkl\n|   ├── cbow_hs.pkl\n|   ├── word2vec_analogy_results.csv\n|   ├── skipgram_ns_tsne.html\n|   └── skipgram_ns_umap.html\n|   ├── skipgram_hs_tsne.html\n|   └── skipgram_hs_umap.html\n|   ├── cbow_ns_tsne.html\n|   └── cbow_ns_umap.html\n|   ├── cbow_hs_tsne.html\n|   └── cbow_hs_umap.html\n├── Task_2\n|   ├── nb_predictions.csv\n|   ├── nb_results.txt\n|  \n└── Task_3\n    └── em_predictions.csv\n```\n\n\nDo **not** include any other files. The grader program unzips and evaluates the output on these files automatically.\n\n\n## Determinism and timing\n\n* Make runs deterministic unless randomness is part of the task (fix a default seed, 42).\n\n## Prohibited items\n\n* data files, readmes, zip-inside-zip, folders.\n* External libraries beyond numpy/nltk/matplotlib/seaborn/pandas.\n* Hard-coded absolute paths, interactive prompts, or manual steps.\n\n## Pre-submission checklist\n\n* [ ] Zip file is named `CS779-A3-[Firstname]-[Lastname]-[Rollno].zip`\n* [ ] Zip contains only the deliverables in the specified directory structure\n* [ ] Output filenames and formats match exactly.\n* [ ] Deterministic behavior (fixed seed where randomness exists).\n\n**Strict enforcement:** The assignments are **graded automatically**. If your zip structure, filenames, arguments, runtime, or outputs do not match the specifications, your submission will **not be evaluated** and will receive **0 marks**.\n","metadata":{"id":"Yoret_qLaQv0"}},{"cell_type":"markdown","source":"#**Enter your details below:**\n\nFull Name: Pranjali Singh\n\nRoll No: 220796\n\nEmail: pranjalis22@iitk.ac.in\n\n","metadata":{"id":"W7Csx9X8aTU7"}},{"cell_type":"markdown","source":"# **Learnings from Assignment:**\n\n\n* Learned to handle overflow/underflow in probability computations (e.g., log-sum-exp trick in EM).\n\n* Saw how learning rate, batch size, and smoothing parameters impact convergence in Word2Vec and EM.\n\n* Explored the effect of number of clusters/components, alpha, max features, n-grams on EM and Naive Bayes.\n\n* Learned what to precompute vs compute on-the-go to save memory and speed up training (e.g., matrix multiplications, dot products, class counts).\n\n* Learned to save intermediate results/checkpoints per epoch for inspection.\n\n* Understood trade-offs: model complexity vs speed, memory vs precomputation, convergence vs overfitting.\n","metadata":{}},{"cell_type":"markdown","source":"# **Introduction**\n\nIn this assignment, we will be exploring implementing feedforward Neural Networks (NN) from scratch. We will be studying NN in the course, however, here is a quick references:\n\n1. https://cs231n.github.io/neural-networks-1/\n2. https://karpathy.github.io/neuralnets/\n3. https://cs231n.github.io/neural-networks-2/\n4. https://cs231n.github.io/optimization-2/\n5. https://jalammar.github.io/illustrated-word2vec/\n\n","metadata":{"id":"4Rn_f3LA92Hw"}},{"cell_type":"markdown","source":"# **Task 1: Word2Vec with Negative Sampling**","metadata":{"id":"ACPLe_gVTWWE"}},{"cell_type":"markdown","source":"## 1. Concept\n\n## **Objective**\nIn this assignment, you will implement the **Word2Vec model from scratch** using the **negative sampling technique**.  \nBy the end of this part, you will:\n- Understand the concept of distributed word embeddings.\n- Learn about **Skip-gram** and **CBOW** architectures.\n- Implement forward pass loss computation, gradient computation, backpropagation, SGD optimization and minibatch-SGD manually.\n- Train Word2Vec on a text corpus with negative sampling.\n- Save trained embeddings into pickle files.\n- Evaluate embeddings on an **analogy task** using a provided dataset.\n\n---\n\n## **1. Introduction to Word2Vec**\n\n### **What is Word2Vec?**\nWord2Vec is a shallow neural network that learns **dense vector representations of words** such that semantically similar words lie close together in the embedding space.\n\nThe key idea: instead of representing words as one-hot vectors, we represent them as low-dimensional **embeddings** learned by predicting context words from a target word (or vice versa).\n\n---\n### Skip-gram Architecture\n\nThe **Skip-gram model** in Word2Vec learns word embeddings by predicting context words given a center (target) word. Instead of using one-hot vectors, we use Label Encoding, which is more memory-efficient and computationally optimal.\n\n---\n\n#### Visual Overview\n\n**1. Skip-gram Model Structure**  \n![word2vec Model architecture](https://miro.medium.com/v2/resize:fit:1400/format:webp/1*tmyks7pjdwxODh5-gL3FHQ.png)\n<!-- ![Skip-gram Model Structure](https://media.geeksforgeeks.org/wp-content/uploads/Skip-gram-architecture-2.jpg) -->\n\n**2. Training Objective Illustration**  \n![Skip-gram Objective](https://media.geeksforgeeks.org/wp-content/uploads/word2vec_diagram-1.jpg)\n\n---\n\n####  Intuition\n\nSentence:  \n**\"The cat sat on the mat\"**, with **window size = 2**\n\nIf target word = `\"cat\"`, context = `[\"The\", \"sat\"]`  \n→ Training pairs:\n- (`cat` → `The`)\n- (`cat` → `sat`)\n\n#### Architecture and Training Flow (with Index-based Input)\n\nLet:  \n- \\( V \\): size of the vocabulary  \n- \\( d \\): embedding dimension  \n-  t in $ 0, 1, \\dots, V-1 $: index of the target word  \n-  $E \\in {R}^{V \\times d}$: input embedding matrix  \n-  $E' \\in {R}^{V \\times d}$: output embedding matrix\n\nThe integer index \\( t \\) is used to perform a direct embedding lookup in matrix \\( E \\), avoiding one-hot vector multiplication.\n\n\n\n\n\n---\n\n####  Step-by-Step Computation:\n\n1. **Input**: Integer ID of target word \\( t \\)\n\n2. **Embedding Lookup** (instead of one-hot):\n\n$$\nu = E[t] \\in \\mathbb{R}^d\n$$\n\n3. **Score for Context Word \\( c \\)**:\n\n$$\n\\text{score}(c) = u^\\top E'[c]\n$$\n\n4. **Softmax over Vocabulary**:\n\n$$\nP(w_c \\mid w_t) = \\frac{\\exp(u^\\top E'[c])}{\\sum_{w=1}^{V} \\exp(u^\\top E'[w])}\n$$\n\n---\n\n\n\n#### Objective Function\n\nGiven a **center word** $ w_t $ , the Skip-gram model aims to **predict surrounding context words** within a fixed window size.\n\n\n$$\n{L}_{\\text{skip-gram}} = \\sum_{-k \\le j \\le k,\\ j \\ne 0} \\log P(w_{t+j} \\mid w_t)\n$$\n\n\nwhere\n\n\n| Term &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;| Description |\n|------|-------------|\n| $ {L}_{{skip-gram}} $ | The **log-likelihood** objective for predicting the context words surrounding the center word $ w_t $ This is the quantity we aim to **maximize** during training. |\n| $ k $| The **window size**, which defines how many words before and after the center word are considered as context. |\n| $ j $ | The **offset** from the center word $ t $, ranging from $ -k $ to $ +k $, excluding $ j = 0 $ (which would be the center word itself). |\n| $ w_t $ | The **center word** (target word) whose surrounding words we are trying to predict. |\n| $ w_{t+j} $ | A **context word** at position $ t+j $ relative to the center word $ w_t $ |\n| $ P(w_{t+j} \\mid w_t) $ | The **conditional probability** that word $ w_{t+j} $ appears in the context of $ w_t $. This is typically modeled using a softmax function or approximated using negative sampling. |\n| $ \\log P(w_{t+j} \\mid w_t) $ | The **log-probability** of correctly predicting a context word. Using the log helps with numerical stability and converts the product of probabilities into a sum. |\n\n\n\n---\n\n\n\n\n#### Parameters Learned\n\n- $ E \\in {R}^{V \\times d} $: Embedding matrix (input)\n- $ E' \\in {R}^{V \\times d} $: Output matrix (context prediction)\n\n**Embedding Lookup** via integer index replaces costly one-hot vector multiplication.\n\n**Note**: Final embeddings are typically taken from matrix $ E $\n---\n\n\n#### Summary\n\n- **Input**: Integer index of target word  \n- **Output**: Predict context word indices  \n- **Lookup**: Directly fetch embedding vector from matrix  \n- **Train**: Using softmax or negative sampling  \n- **Goal**: Words appearing in similar contexts should have similar vectors\n\n---\n\n\n\n### CBOW Architecture\n\nThe **CBOW model** in Word2Vec learns word embeddings by predicting a center (target) word given its surrounding context words. Like Skip-gram, it avoids one-hot vectors using index-based embedding lookup, making it both space- and time-efficient.\n\n---\n\n#### Visual Overview\n\n**1. CBOW Model Structure**  \n![CBOW Model Structure](https://media.licdn.com/dms/image/v2/D4D12AQG9BzheEOLwpg/article-cover_image-shrink_720_1280/article-cover_image-shrink_720_1280/0/1710832109081?e=1760572800&v=beta&t=3W-eSSjvNqSbz-AKtefhhbQXDFizyohSjyAWaZLek1I)\n\n\n\n---\n\n#### Intuition\n\nSentence:  \n**\"The cat sat on the mat\"**, with **window size = 2**\n\nIf target word = `\"cat\"`, context = `[\"The\", \"sat\"]`  \n→ Training pairs:\n- (`[\"The\", \"sat\"]` → `cat`)\n\n#### Architecture and Training Flow (with Index-based Input)\n\nLet:  \n- $ V $: size of the vocabulary  \n- $ d $: embedding dimension  \n-   C = $ c_1, c_2, \\dots, c_m $ : list of context word indices  \n-  $ t $: index of the target word  \n-  $ E \\in {R}^{V \\times d} $ : input embedding matrix  \n-  $ E' \\in {R}^{V \\times d} $ : output embedding matrix\n\nEach context word $ c_i \\in C $ is used to lookup a vector from $ E $, which are then averaged to form a single context embedding.\n\n---\n\n#### Step-by-Step Computation:\n\n1. **Input**: Integer IDs of context words  C = $ {c_1, c_2, \\dots, c_m} $\n\n2. **Embedding Lookup**:\n\n$$\nh = \\frac{1}{m} \\sum_{i=1}^{m} E[c_i] \\in \\mathbb{R}^d\n$$\n\n3. **Score for Target Word $ t $**:\n\n$$\n\\text{score}(t) = h^\\top E'[t]\n$$\n\n4. **Softmax over Vocabulary**:\n\n$$\nP(w_t \\mid \\text{context}) = \\frac{\\exp(h^\\top E'[t])}{\\sum_{w=1}^{V} \\exp(h^\\top E'[w])}\n$$\n\n---\n\n#### Objective Function\n\nGiven **context words** $ \\{w_{t-k}, \\dots, w_{t-1}, w_{t+1}, \\dots, w_{t+k}\\} $, the CBOW model aims to **predict the center word** \\( w_t \\).\n\n$$\n{L}_{\\text{CBOW}} = \\log P(w_t \\mid w_{t-k}, \\dots, w_{t-1}, w_{t+1}, \\dots, w_{t+k})\n$$\n\nwhere\n\n\n| Term &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;    | Description |\n|---------|-------------|\n| $ {L}_{\\text{CBOW}} $ | The **log-likelihood** objective for predicting the center word $ w_t $ given its surrounding context words. |\n| $ k $ | The **window size**, which defines how many words before and after the target word are used as context. |\n| $ c_i $ | The **context word index** used for embedding lookup. |\n| $ w_t $ | The **center word** (target word) to be predicted. |\n| $ P(w_t \\mid \\text{context}) $ | The **conditional probability** of $ w_t $ being the correct word, given the context. |\n| $ log P(w_t \\mid \\text{context}) $ | The **log-probability** of predicting $ w_t $ correctly given its context. |\n\n\n---\n\n#### Parameters Learned\n\n- $ E \\in {R}^{V \\times d} $: Embedding matrix (input)\n- $ E' \\in {R}^{V \\times d} $: Output matrix (used for prediction)\n\n**Note**: Final embeddings are typically taken from matrix $ E $\n\n---\n\n### Summary\n\n- **Input**: Integer indices of context words  \n- **Output**: Predict index of target (center) word  \n- **Lookup**: Embed each context word, then average  \n- **Train**: With softmax or negative sampling  \n- **Goal**: Words that appear in similar contexts should learn similar vector representations\n\n---\n\n### **Negative Sampling in Word2Vec**\n\n---\n\n#### Why Not Full Softmax?\n\nIn the original Skip-gram formulation, the softmax function is used to compute the probability of a context word given a target word:\n\n$$\nP(w_o \\mid w_t) = \\frac{\\exp(u_{w_o}^\\top v_{w_t})}{\\sum_{w=1}^{V} \\exp(u_w^\\top v_{w_t})}\n$$\n\nWhere:\n- $ v_{w_t} $: input (center) embedding of word $ w_t $\n- $ u_{w_o} $: output (context) embedding of word $ w_o $\n- $ V $: vocabulary size\n\n**Problem**:  \nThe denominator sums over **all words in the vocabulary** $ V $ — this is extremely slow for large corpora.\n\n---\n\n#### Solution: Negative Sampling\n\nInstead of updating **all output weights**, **update only a small number of \"negative samples\"** along with the one positive pair.\n\n---\n\n#### Intuition\n\n- For each training pair $ (w_t, w_o) $:\n  - Treat it as a **positive example**\n  - Sample $ k $ random words $ \\{w_1^{'}, w_2^{'}, ..., w_k^{'}\\} $ from the vocabulary — these are **negative samples**\n- The goal:\n  - **Maximize** the probability of real context word $ w_o $\n  - **Minimize** the probability of negative samples $ w_i^{'} $\n\n---\n\n#### Loss Function\n\nFor a center word $ w_t $ and a context word $ w_o $, the **negative sampling loss** is:\n\n$$\n{L}_{\\text{negative-sampling}} = -\\log \\sigma(u_{w_o}^\\top v_{w_t}) - \\sum_{i=1}^{k} \\log \\sigma(-u_{w_i^{'}}^\\top v_{w_t})\n$$\n\nWhere:\n- $ \\sigma(x) = \\frac{1}{1 + \\exp(-x)} $  : sigmoid function\n- $ u_w $: output embedding  of word $ w $\n- $ v_w $: input embedding  of word $ w $\n- $ w_o $: actual word (positive sample)\n- $ w_i^{'} $: i-th negative sample\n- $ k $: number of negative samples\n\n---\n\n#### Training Workflow (Step-by-Step)\n\n1. **Input**: target word $ w_t $, context word $ w_o $\n2. **Get embeddings**:\n   - $ v_{w_t} \\in {R}^d $ from input matrix $ E $\n   - $ u_{w_o} \\in {R}^d $ from output matrix $ E'$\n3. **Sample** $ k $ random words from vocabulary as negative samples\n4. **Compute loss** using the formula above\n5. **Backpropagate** and update only:\n   - $ v_{w_t} $\n   - $ u_{w_o} $\n   - $ u_{w_i^{'}} $ for all $ i \\in [1, k] $\n\n---\n\n\n\n\n#### Why It Works\n\n- Drastically reduces **computation time**.\n- Still allows the model to learn **good semantic embeddings**.\n- Scales to **billions of tokens** and **millions of words**.\n- Enables **mini-batch training** with sparse updates.\n\n\n---\n### **Hierarchical Softmax in Word2Vec**\n\n---\n\n####  Why Another Alternative to Softmax?\n\nLike Negative Sampling, **Hierarchical Softmax (HS)** addresses the computational inefficiency of the full softmax, especially for large vocabularies.\n\n**Key Idea**:  \n Instead of scoring all words in the vocabulary, we build a **binary tree** and compute the probability of a word by traversing the path from the **root to that word's leaf**.\n\n---\n\n#### How Hierarchical Softmax Works\n\n1. Build a **binary tree** where:\n   - **Leaves** represent vocabulary words.\n   - **Internal nodes** make **binary decisions**: left/right.\n\n2. Each word is uniquely identified by a **path from the root to a leaf**.\n\n3. The model:\n   - Learns vector representations for each **internal node**.\n   - Predicts the correct **binary decision** at each node in the path.\n\n---\n\n#### Example\n\nSay the word **\"sat\"** is located along this binary path:\n```\nRoot → Left → Right → Left → [sat]\n```\n\nWe must predict:\n- Step 1: Go **Left**\n- Step 2: Go **Right**\n- Step 3: Go **Left**\n\nSo the model predicts **a sequence of decisions** — **not** the word directly.\n\n---\n\n#### Mathematical Formulation\n\nLet:\n- $ w $ : target word to predict\n- $ w_t $ : input word\n- $ L(w) $ : length of the binary path to $ w $\n- $ n_i $ : the $ i^{th} $ internal node along the path to $ w $\n- $ s_i \\in \\{+1, -1\\} $: direction at node $ n_i $ (+1 = left, -1 = right)\n- $ v_{w_t} $ : input vector for $ w_t $\n- $ v_{n_i} $ : vector associated with internal node $ n_i $\n- $ \\sigma(x) = \\frac{1}{1 + \\exp(-x)} $\n\nThen:\n\n$$\nP(w \\mid w_t) = \\prod_{i=1}^{L(w)} \\sigma\\left( s_i \\cdot v_{n_i}^\\top v_{w_t} \\right)\n$$\n\n---\n\n#### Loss Function\n\nFor predicting word $ w $ given center word $ w_t $, the **loss is**:\n\n$$\n{L}_{\\text{HS}} = - \\sum_{i=1}^{L(w)} \\log \\sigma\\left( s_i \\cdot v_{n_i}^\\top v_{w_t} \\right)\n$$\n\n- Similar to Negative Sampling, the model uses **sigmoid** activations at each node.\n- Each decision is treated as a **binary classification task**.\n\n---\n\n\n#### Summary\n\n- Hierarchical Softmax is a tree-based approximation to full softmax.\n- Words are predicted by following binary paths.\n- Each step in the path is trained via sigmoid + binary cross-entropy.\n- Enables fast, full-vocab prediction with logarithmic complexity.\n\n---\n\n","metadata":{"id":"GR3UXxEJacNs"}},{"cell_type":"markdown","source":"## **2. Task Explanation [Implementation - $200$ marks]**\n\n### **Goal**:\n\n- Implement **Word2Vec** from scratch in Python using **NumPy only** (no external ML/DL libraries such as PyTorch or TensorFlow).\n- Use the below code to download the dataset for training and testing.\n  ```python\n  # Expectation-Maximization\n  word2vec_train = load_dataset(\"Exploration-Lab/CS779-Fall25\", \"Assignment-3-word2vec\", split=\"train\")\n  word2vec_val = load_dataset(\"Exploration-Lab/CS779-Fall25\", \"Assignment-3-word2vec\", split=\"val\")\n  ```\n- Support both **Skip-Gram** and **CBOW** architectures.\n- Implement two training strategies for each:\n  - **Negative Sampling**\n  - **Hierarchical Softmax**\n\n- After training your Word2Vec model, save the learned embeddings along with the vocabulary mappings using `pickle`:\n\n  ```python\n  import pickle\n\n  with open(\"embeddings.pkl\", \"wb\") as f:\n      pickle.dump({\n          \"embeddings\": embeddings,     # numpy  array of shape (vocab_size, embedding_dim)\n          \"vocab\": vocab.itos,          # list: index → token\n          \"stoi\": vocab.stoi            # dict: token → index\n      }, f)\n  ```\n\n- Evaluate the learned embeddings using a **word analogy task** and generate a result CSV file with predictions from all four models.\n\n\n**Important**: Every single component of the algorithm — forward pass, backward pass, gradient calculation, parameter updates, hierarchical softmax (Huffman Tree), negative sampling, and training loop — must be written from scratch **without using any external machine learning or deep learning libraries**.\n\n\n---\n\n### **Implementation Requirements**\n\n#### **1. Preprocessing**\n- Use `nltk.word_tokenize` for tokenization.\n- Lowercase all tokens.\n- Apply minimum frequency pruning: `min_count = 5`.\n- Construct vocabulary such that the most frequent word gets index `0`, next most frequent gets `1`, and so on.\n\n#### **2. Training Configuration**\n- Embedding Dimension: `100`\n- Sliding Window Size: `2`\n- Random Seed: `42` (ensure deterministic initialization for reproducibility)\n- Architecture Modes:  \n  - `Skip-Gram with Negative Sampling`  \n  - `Skip-Gram with Hierarchical Softmax`  \n  - `CBOW with Negative Sampling`  \n  - `CBOW with Hierarchical Softmax`  \n\n\n#### **3. You MUST implement the following from scratch:**\n- Vocabulary construction with frequency-based indexing\n- Forward pass using dot product between input and output embeddings\n- Backward pass with gradient computation and update rules\n- **Negative Sampling**:\n  - Use a noise distribution proportional to\n\n     $$\n    \\text{Unigram}^{0.75}\n     $$\n\n\n  - Sample negative examples independently\n- **Hierarchical Softmax**:\n  - Construct a **Huffman Tree** based on word frequencies\n  - Compute paths and binary codes for each word\n  - Implement internal node updates and loss calculations\n- Optimization using Mini Bacth stochastic gradient descent (SGD)\n\n\n\n### **3. Analogy Evaluation**\n\nUse the provided **analogy dataset** where each line is of the format:\n\n```\nword1 + word2 - word3 = ?\n```\n\n- For each trained model, compute:\n  \n\n  $$\n  \\vec = \\text{embedding}[\\text{word1}] + \\text{embedding}[\\text{word2}] - \\text{embedding}[\\text{word3}]\n  $$\n- Predict the closest word in embedding space to `vec` (excluding word1, word2, word3).\n- Save predictions in a CSV with the following format:\n\n```\nword1,op1,word2,op2,word3,skipgram_ns_word,skipgram_hs_word,cbow_ns_word,cbow_hs_word\n```\n\n\n---\n\n\n\n\n### **4. Semantic Neighborhood Visualization using t-SNE and UMAP (with Plotly)**\n\nIn this section, you will explore the **semantic structure of your learned embeddings** by visualizing neighborhoods of similar words using t-SNE and UMAP.\n\n---\n\n####  **Objective**  \nVisualize semantically similar clusters by projecting 500 word vectors into 2D space using t-SNE and UMAP.\n\n---\n\n####  Architectures to visualize:\n- CBOW + Negative Sampling\n- CBOW + Hierarchical Softmax\n- Skip-gram + Negative Sampling\n- Skip-gram + Hierarchical Softmax\n\n---\n\n####  **Steps to Follow**\n\n1. **Randomly Sample 50 Words**\n   - From your vocabulary, randomly select 50 unique words that:\n     - Are not special tokens like `<unk>` or `<pad>`\n\n2. **Find Top 10 Most Similar Words (Cosine Similarity)**\n   - For each of the 50 sampled words:\n     - Compute cosine similarity with **all words in the vocabulary**\n     - Select the top 10 most similar words (excluding the word itself)\n   - Total words = 50 × 10 = **500 word vectors**\n\n3. **Extract Embeddings**\n   - Collect the embeddings for all 500 words.\n   - Store the word labels for plotting.\n\n4. **t-SNE and UMAP Reduction**\n   - Apply **t-SNE** and **UMAP** to project the 500 embeddings to 2D.\n   - Use:\n     - `sklearn.manifold.TSNE(n_components=2)`\n     - `umap.UMAP(n_components=2)`\n\n5. **Visualize using Plotly**\n   - Create **interactive scatter plots** using `plotly.express.scatter`\n   - Include:\n     - Word label as hover tooltip\n     - Title as model type + method (e.g., `t-SNE | Skip-gram + HS`)\n     - Save as `.html`\n\n6. **Save Output**\n   - Save the plots with descriptive names like:\n     - `skipgram_ns_tsne.html`\n     - `skipgram_ns_umap.html`\n\n---\n\n### **5. Deliverables**\n\n- **Embeddings**: Save the final word embeddings as four `.pkl` files:\n  - `skipgram_ns.pkl`\n  - `skipgram_hs.pkl`\n  - `cbow_ns.pkl`\n  - `cbow_hs.pkl`\n  \n\n- **Results**: Save the analogy task predictions in a CSV file:\n  - `word2vec_analogy_results.csv`\n\n\n- **Plots** : 8 Plots of T-sne and UMAP\n  - `skipgram_ns_tsne.html`\n  - `skipgram_ns_umap.html`\n\n  Similar naming convention for the other plots.\n\n\n---\n**Note**: This assignment is a way to explore various trajectories for a given problem. Clarifying every single minute detail about the implementation like hyperparameters, tolerance limit for early stopping etc. will not be entertained on Discord. You can always explore multiple paths and select the most suitable solution for the assignment. You can make assumptions about the implementation details and document it in the code. It will be highly rewarded.\n\n---\n\n### **Additional Suggestions to Fix for Determinism & Reproducibility**\n\n- Use `np.random.seed(42)` globally for all randomness (weight init, sampling).\n\n---\n\n### **Operating Constraints**\n\n- DO NOT use libraries like PyTorch, TensorFlow, Gensim.\n- ONLY use **Python standard library**  , **NumPy** , **Pandas**  , **Plotly** ,  **scikit-learn** .\n\n- Ensure clear, well-commented code and separate training routines for each mode.\n\n---","metadata":{"id":"s1hyWis1Tn17"}},{"cell_type":"code","source":"import math, os, random, pickle, sys\nfrom collections import Counter, defaultdict\nfrom typing import List, Tuple, Dict\nimport numpy as np\nimport pandas as pd\nfrom tqdm.auto import tqdm\nfrom sklearn.manifold import TSNE\nimport umap as umap_lib\nimport plotly.express as px\nSEED = 42\nnp.random.seed(SEED)\nrandom.seed(SEED)\nNEGATIVE_SAMPLES = 5 # required\nOUT_ROOT = \"Task_1\"\nos.makedirs(OUT_ROOT, exist_ok=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-30T15:27:53.924221Z","iopub.execute_input":"2025-09-30T15:27:53.924484Z","iopub.status.idle":"2025-09-30T15:28:49.219271Z","shell.execute_reply.started":"2025-09-30T15:27:53.924456Z","shell.execute_reply":"2025-09-30T15:28:49.218269Z"}},"outputs":[{"name":"stderr","text":"2025-09-30 15:28:18.502125: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1759246098.810371      36 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1759246098.912775      36 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import nltk\nfrom datasets import load_dataset\nfrom nltk.tokenize import word_tokenize\nnltk.download(\"punkt\", quiet=True)\nnltk.download(\"punkt_tab\", quiet=True)\nhf_token = \"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\"\nREPO_ID = \"Exploration-Lab/CS779-Fall25\"\nEMBEDDING_DIM = 100\nMIN_COUNT = 5\nWINDOW_SIZE = 2\ndef load_hf_data() -> List[List[str]]:\n    print(\"Loading and tokenizing dataset...\")\n    ds = load_dataset(\"Exploration-Lab/CS779-Fall25\", \"Assignment-3-word2vec\", split=\"train\",token=hf_token, trust_remote_code=True)\n    text_data = ds[\"text\"]\n    return [word_tokenize(doc.strip().lower()) for doc in tqdm(text_data, desc=\"Tokenizing Sentences : \") if doc.strip()]\ndata = load_hf_data()","metadata":{"id":"wOeI4kgEpVvM","trusted":true,"execution":{"iopub.status.busy":"2025-09-30T12:41:15.180514Z","iopub.execute_input":"2025-09-30T12:41:15.180946Z","iopub.status.idle":"2025-09-30T12:42:13.472384Z","shell.execute_reply.started":"2025-09-30T12:41:15.180928Z","shell.execute_reply":"2025-09-30T12:42:13.471501Z"}},"outputs":[{"name":"stdout","text":"Loading and tokenizing dataset...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"README.md:   0%|          | 0.00/6.20k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b3ad8585dee04a14a065392a24397269"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Assignment-3/word2vec/train.parquet:   0%|          | 0.00/34.1M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d4231576fc6d4936ba9391307bd885f5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/200000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"70e91de587de4832b7159793e185e3c4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Tokenizing Sentences :   0%|          | 0/200000 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"50d6ac2a685b4ff087399ad6f1893c2b"}},"metadata":{}}],"execution_count":2},{"cell_type":"code","source":"class Vocab:\n    def __init__(self, sentences, min_count):\n        # Get the frequency of all words in the dataset\n        word_counts = Counter()\n        for sentence in sentences:\n          for word in sentence:\n        # Update the count for each word\n            word_counts[word]+=1\n        # Store the total number of tokens before any filtering\n        self.total_dataset_tokens = sum(word_counts.values())\n        # Filter out words that don't meet the min_count threshold\n        self.words=[]  \n        for word, count in word_counts.items():\n    # check if this word meets the minimum count threshold\n         if count>= min_count:\n          self.words.append(word)\n        # Sort the vocabulary. We want the most frequent words to have the lowest indices\n        # For words with the same frequency,sorting them alphabetically for consistency\n        self.words.sort(key=lambda word: (-word_counts[word], word))\n# Create the word-to-index and index-to-word mappings\n        self.word2idx= {word: i for i, word in enumerate(self.words)}\n        self.idx2word = self.words # The list itself serves as the index-to-word mapping\n               # Store the counts for the words that are actually in our final vocab\n        self.counts =np.array([word_counts[w] for w in self.idx2word], dtype=np.int64)\n        self.vocab_size =len(self.idx2word)\n        print(f\"Vocabulary built with {self.vocab_size} unique words.\")\n    def get_id(self, word: str) -> int | None:\n        return self.word2idx.get(word)\nvocab =Vocab(data, min_count=MIN_COUNT)","metadata":{"id":"zR7S2hZ00f3s","trusted":true,"execution":{"iopub.status.busy":"2025-09-30T12:42:13.474250Z","iopub.execute_input":"2025-09-30T12:42:13.474871Z","iopub.status.idle":"2025-09-30T12:42:16.124948Z","shell.execute_reply.started":"2025-09-30T12:42:13.474848Z","shell.execute_reply":"2025-09-30T12:42:16.124235Z"}},"outputs":[{"name":"stdout","text":"Vocabulary built with 54298 unique words.\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"def build_skipgram_pairs(sentences, vocab, window):\n    pairs= []\n    for sent in tqdm(sentences, desc=\"skip-gram\"):\n        ids =[vocab.get_id(w) for w in sent if vocab.get_id(w) is not None]\n        for pos, center in enumerate(ids): # Collecting context words within the specified window (excluding center)\n            for ctx_pos in range(max(0, pos-window), min(len(ids), pos+window+1)):\n                if ctx_pos!=pos:\n                 pairs.append((center, ids[ctx_pos])) # (center, context) format\n    return pairs\ndef build_cbow_examples(sentences, vocab, window):\n    examples =[]\n    for sent in tqdm(sentences, desc=\"cbow\"):\n        ids =[vocab.get_id(w) for w in sent if vocab.get_id(w) is not None]\n        for pos, target in enumerate(ids):  #Gather context words around the target word\n            ctx= [ids[i] for i in range(max(0, pos-window), min(len(ids), pos+window+1)) if i != pos]\n            if ctx:\n             examples.append((ctx, target)) # ([context words], target) format\n    return examples\nsg_pairs = build_skipgram_pairs(data, vocab, WINDOW_SIZE)\ncbow_examples = build_cbow_examples(data, vocab, WINDOW_SIZE)\nprint(\"SG pairs:\", len(sg_pairs), \"CBOW examples:\", len(cbow_examples))","metadata":{"id":"-fgEhbYH0gDI","trusted":true,"execution":{"iopub.status.busy":"2025-09-30T12:42:16.125629Z","iopub.execute_input":"2025-09-30T12:42:16.125882Z","iopub.status.idle":"2025-09-30T12:43:10.801760Z","shell.execute_reply.started":"2025-09-30T12:42:16.125864Z","shell.execute_reply":"2025-09-30T12:43:10.800862Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"skip-gram:   0%|          | 0/129240 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8a63dec0f7e94772966e1eed4beb3bad"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"cbow:   0%|          | 0/129240 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b1e4a188b083400cbc6c4a0131c37a06"}},"metadata":{}},{"name":"stdout","text":"SG pairs: 44815576 CBOW examples: 11396863\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"class NegativeSampler:\n    def __init__(self, probabilities, seed=SEED):\n        # prob should sum up to 1\n        probs =np.array(probabilities, dtype=np.float64)\n        probs= probs/probs.sum()\n        self.K =len(probs)\n        # scaling probabilities by K \n          # Alias method works by splitting each \"bucket\" into exactly 1 unit of mass.\n        # Some buckets will be <1.0, others >1.0\n        scaled_probs =probs*self.K   # Intuition: we want to split probability mass into \"buckets\" of size 1/K.\n        self.prob =np.zeros(self.K, dtype=np.float64)   # self.prob[i]  = adjusted probability for bucket i\n                                                   # self.alias[i] = alternate bucket to use if coin flip fails\n        self.alias= np.zeros(self.K, dtype=np.int32)\n        small =[]  # Buckets to track which indices are \"small\" (<1) vs \"large\" (>1).\n        large =[]   # If scaled_probs[i] < 1 then bucket has less than average mass\n                                                   # If scaled_probs[i] > 1 then bucket has more than average mass\n        for i,p in enumerate(scaled_probs):\n         if p < 1.0:\n            small.append(i)\n         else:\n            large.append(i)\n    # we should  pair one underfull bucket with one overfull bucket.\n        # Assigning probability mass and redirect leftover probability through alias.\n        while small and large:\n            small_idx =small.pop()\n            large_idx= large.pop()\n            self.prob[small_idx]= scaled_probs[small_idx] # small bucket gets its actual probability\n            self.alias[small_idx] = large_idx # if coin flip fails,we jump  to this \"large\" bucket\n            # adjusting the leftover probability\n            scaled_probs[large_idx] = scaled_probs[large_idx]-(1.0-scaled_probs[small_idx]) # Reducing leftover mass of the large bucket\n            if scaled_probs[large_idx]<1.0:\n             small.append(large_idx) # Think again if the large bucket if it now became \"small\"\n            else:\n             large.append(large_idx)\n        for idx in small+large: #Any leftover buckets are perfectly filled (prob = 1)\n         self.prob[idx]=1.0\n         self.alias[idx] = idx\n        # random number generator\n        self.rng = np.random.default_rng(seed)\n    def sample(self, n_samples):\n           # Step A: pick a random bucket uniformly\n        indices = self.rng.integers(0, self.K, size=n_samples)\n        coins = self.rng.random(size=n_samples)  # Step B: flip a biased coin (based on precomputed self.prob)\n        use_alias= coins>=self.prob[indices]  # Step C: if coin fails then we use alias (fallback bucket)\n        samples =indices.copy()\n        samples[use_alias] = self.alias[indices[use_alias]]\n        return samples\nfreqs = vocab.counts.astype(np.float64)\nneg_probs = freqs ** 0.75\nneg_probs=neg_probs / neg_probs.sum()\nneg_sampler = NegativeSampler(neg_probs, seed=SEED)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-30T12:43:10.805161Z","iopub.execute_input":"2025-09-30T12:43:10.805384Z","iopub.status.idle":"2025-09-30T12:43:10.870405Z","shell.execute_reply.started":"2025-09-30T12:43:10.805366Z","shell.execute_reply":"2025-09-30T12:43:10.869843Z"}},"outputs":[],"execution_count":5},{"cell_type":"markdown","source":"https://medium.com/@tnodecode/word2vec-negative-sampling-63995b13464f","metadata":{}},{"cell_type":"code","source":"import heapq\ndef build_huffman_tree(counts: np.ndarray):\n    class Node:\n        def __init__(self, freq: int, wid: int = None, left=None, right=None):\n            self.freq =freq  # Each node holds a frequency (sum of word counts for its subtree)\n            # wid is only set for leaf nodes (actual vocabulary entries).\n            self.wid = wid       # word index for leaves; None for internal nodes\n            self.left= left\n            self.right = right\n    # initializing heap with (freq, tie_id, node)\n    heap =[]\n    for i, f in enumerate(counts):\n        node =Node(int(f),wid=i)  # leaf node for word i\n        heap.append((int(f),i,node)) # tie_breaker (i) ensures deterministic merges when freqs are equal.\n    heapq.heapify(heap)\n    # merge until single root remains\n    next_tie= len(heap)\n    while len(heap) > 1:\n        f1, _, n1 =heapq.heappop(heap)\n        f2, _, n2= heapq.heappop(heap)\n        merged = Node(f1+f2, wid=None, left=n1, right=n2)   # Merge two smallest nodes: new internal node with combined frequency\n        heapq.heappush(heap, (merged.freq,next_tie, merged))\n        next_tie+=1\n    root =heap[0][2]  # The single element left is the root of the Huffman tree\n    # assign contiguous IDs to internal nodes (preorder / any deterministic traversal)\n    internal_id_map = {}\n    internal_counter = 0\n    # This is helpful because later, hierarchical softmax wants\n    # a dense set of internal-node IDs (for indexing into weight matrices).\n    def assign_ids(node):\n        nonlocal internal_counter\n        if node is None:\n            return\n        if node.wid is None:\n            internal_id_map[id(node)]=internal_counter\n            internal_counter+=1\n            assign_ids(node.left)\n            assign_ids(node.right)\n    assign_ids(root)\n    # traverse leaves to build codes and paths\n    # codes[word] = list of 0/1 Huffman bits\n    # paths[word] = list of internal node IDs corresponding to the decision path\n    codes = {}\n    paths = {}\n    def walk(node, code_so_far, internal_path):\n        if node is None:\n            return\n        if node.wid is not None:\n             # Found a leaf = a word\n            codes[node.wid] =list(code_so_far)   # copy\n            # convert node objects in internal_path -> their assigned contiguous ids\n            paths[node.wid]=[internal_id_map[id(n)] for n in internal_path]\n            return\n        # go left -> bit 0\n        walk(node.left,code_so_far+[0], internal_path+[node])\n        # go right -> bit 1\n        walk(node.right, code_so_far+[1], internal_path+[node])\n    walk(root,[],[])\n    n_internal=internal_counter\n    return codes,paths,n_internal\nhuff_codes, huff_paths, huff_n_internal = build_huffman_tree(vocab.counts)\nprint(\"Huffman built, internal nodes:\", huff_n_internal)","metadata":{"id":"7W1UW8PC0gj5","trusted":true,"execution":{"iopub.status.busy":"2025-09-30T12:43:39.568971Z","iopub.execute_input":"2025-09-30T12:43:39.569679Z","iopub.status.idle":"2025-09-30T12:43:39.958694Z","shell.execute_reply.started":"2025-09-30T12:43:39.569654Z","shell.execute_reply":"2025-09-30T12:43:39.958087Z"}},"outputs":[{"name":"stdout","text":"Huffman built, internal nodes: 54297\n","output_type":"stream"}],"execution_count":6},{"cell_type":"markdown","source":"https://www.geeksforgeeks.org/dsa/huffman-coding-in-python/","metadata":{}},{"cell_type":"code","source":"class Word2Vec:\n    def __init__(self, vocab, dim=EMBEDDING_DIM, lr=LR):\n        self.vocab = vocab\n        self.dim = dim\n        self.lr= lr\n        V = vocab.vocab_size\n        # input embedding matrix\n        #The idea is to initialize weights with small, random values to prevent gradients from immediately vanishing or exploding.\n        limit = np.sqrt(6.0 / (V + self.dim)) # (earlier I didn't do this but my gradients were increasing hugely)\n        self.W_in = np.random.uniform(-limit, limit, (V, self.dim))\n        # output embedding matrix for negative sampling\n        self.W_out = np.zeros((V,self.dim))\n        self.W_internal =None\n        self.huff_codes= None\n        self.huff_paths =None\n        self.n_internal= 0\n    def init_hs(self, codes, paths, n_internal):\n        self.huff_codes=codes\n        self.huff_paths=paths\n        self.n_internal= n_internal\n        self.W_internal =(np.random.randn(n_internal, self.dim)*0.01).astype(np.float32)\n        # A simple small random initialization works well here. I'm scaling by 0.01\n        # to keep the initial dot products small and stable.\n        # Keeping internal embeddings separate for hierarchical softmax\n    @staticmethod\n    def sigmoid(x):\n    # vectorized stable sigmoid\n      out = np.zeros_like(x, dtype=float)\n      pos = x >= 0\n      neg = ~pos\n    # for positive x, avoiding overflow in exp(-x)\n      out[pos] = 1 / (1 + np.exp(-x[pos]))\n    # for negative x, to avoid overflow in exp(x)\n      exp_x = np.exp(x[neg])\n      out[neg] = exp_x / (1 + exp_x)\n      return out\n\n    def train_skipgram_ns_batch(self, centers_np, contexts_np, neg_sampler, K, score_clip=25.0):\n     B = len(centers_np)\n     emb_centers =self.W_in[centers_np]    # (B, D)\n     emb_contexts = self.W_out[contexts_np] # (B, D) Using einsum for clean, efficient vectorized computation.\n     pos_scores =np.einsum('bd,bd->b', emb_centers, emb_contexts, optimize=True)  # Positive scores: dot product of center and context embeddings\n     pos_scores = np.clip(pos_scores, -score_clip, score_clip)  # Clipping scores to avoid overflow in exp/log computations\n     loss_pos= np.sum(np.logaddexp(0.0, -pos_scores))\n     g_pos = (self.sigmoid(pos_scores)-1.0)[:, None] # (B, 1))   # Gradient wrt positive scores\n     neg_samples =neg_sampler.sample(B*K).reshape(B, K)\n     emb_negs =self.W_out[neg_samples]             # (B, K, D)\n     neg_scores = np.einsum('bkd,bd->bk', emb_negs, emb_centers, optimize=True)\n     neg_scores = np.clip(neg_scores, -score_clip, score_clip)  # Computig scores for negative samples\n     loss_neg= np.sum(np.logaddexp(0.0, neg_scores))\n     g_negs = self.sigmoid(neg_scores)              # (B, K)\n     grad_out_pos= g_pos*emb_centers  # Gradient update for positive context embeddings\n        # It's possible for the same word to appear multiple times in a batch (e.g., as a\n        # positive and negative sample). A simple `W_out[indices] -= ...` would lead to a\n        # race condition. `np.add.at` performs update, ensuring gradients\n        # for the same word are correctly accumulated.\n     np.add.at(self.W_out, contexts_np, -self.lr*grad_out_pos)\n     grad_out_negs_einsum = np.einsum('bk,bd->bkd', g_negs, emb_centers, optimize=True) # Gradient update for negative embeddings \n     np.add.at(self.W_out, neg_samples.flatten(), -self.lr*grad_out_negs_einsum.reshape(-1, self.dim))\n     grad_in_pos = g_pos*emb_contexts     # Gradient update for center embeddings\n     grad_in_negs =np.einsum('bk,bkd->bd', g_negs, emb_negs, optimize=True)\n     grad_in = grad_in_pos+grad_in_negs\n     np.add.at(self.W_in, centers_np, -self.lr * grad_in)\n     return float((loss_pos+loss_neg)/B)\n\n    def train_skipgram_hs_batch(self, batch_pairs):\n     B =len(batch_pairs)\n     center_indices = np.array([p[0] for p in batch_pairs], dtype=np.int32)\n     context_indices = np.array([p[1] for p in batch_pairs], dtype=np.int32)\n        #The strategy here is to pad them all huffman paths to length of the longest path in the batch, do all calculations in one big NumPy operation,\n        # and then use a `mask` to ignore the results from the padded parts. This is the key\n        # to making HS fast.\n     paths= [self.huff_paths[i] for i in context_indices]\n     codes= [self.huff_codes[i] for i in context_indices]\n     path_lens = np.array([len(p) for p in paths], dtype=np.int32)\n     max_path_len =np.max(path_lens)\n     row_indices =np.repeat(np.arange(B), path_lens)\n     col_indices= np.concatenate([np.arange(l) for l in path_lens])\n # Flatten variable-length paths for batch processing\n     paths_padded= -np.ones((B, max_path_len), dtype=np.int32)\n     codes_padded= -np.ones((B, max_path_len), dtype=np.int32)\n # Pad paths and codes with -1\n     paths_padded[row_indices, col_indices] = np.concatenate(paths)\n     codes_padded[row_indices, col_indices] = np.concatenate(codes)\n     mask=(paths_padded != -1)\n     v_centers = self.W_in[center_indices] #  embeddings for center words\n     internal_indices_safe = np.maximum(0, paths_padded) # Avoids index error from -1 padding\n     u_nodes = self.W_internal[internal_indices_safe]\n     scores = np.einsum('bd,bpd->bp', v_centers, u_nodes, optimize=True) # Compute scores along Huffman paths\n     sigmoids = self.sigmoid(scores)\n     loss_terms = np.where(\n        codes_padded == 1,   # loss per path \n        -np.log(sigmoids+1e-9),  # Increased epsilon slightly for stability\n        -np.log(1-sigmoids+1e-9)\n    )\n     total_loss = np.sum(loss_terms*mask)# The mask ensures we don't sum loss from padded nodes.\n     grads =sigmoids-(1-codes_padded)  # Gradient w.r.t scores\n     grads *= mask # ignore padded positions\n    # Update W_internal using einsum (can be slightly faster)   (the inner HS nodes)\n     grad_W_internal = np.einsum('bp,bd->bpd', grads, v_centers)\n     valid_paths = paths_padded[mask]\n     valid_grads = grad_W_internal[mask]\n     np.add.at(self.W_internal, valid_paths, -self.lr*valid_grads)\n    # Update W_in using einsum\n     grad_W_in = np.einsum('bp,bpd->bd', grads, u_nodes)\n    # Use np.add.at for W_in as well, in case a center word appears multiple times\n     np.add.at(self.W_in, center_indices, -self.lr*grad_W_in)\n     return np.nan_to_num(total_loss/B)\n\n    def train_cbow_hs_batch(self, batch_idx,\n                            contexts_padded_full, ctx_lens_full, all_targets,\n                            paths_nodes_full, paths_bits_full, path_lens_full):\n\n     contexts_padded = contexts_padded_full[batch_idx]  # (B, max_ctx)\n     ctx_lens = ctx_lens_full[batch_idx]                # (B,)\n     targets = all_targets[batch_idx]                   # (B,)\n     B = len(batch_idx)\n     max_ctx = contexts_padded.shape[1]\n     dim = self.dim\n    # compute v_context (B, dim) (masked mean)\n     mask_ctx = (contexts_padded != -1)                 # (B, max_ctx)\n     emb_ctxs = self.W_in[contexts_padded.clip(0)]      # padded -1 -> 0 index, then masked\n     emb_ctxs = emb_ctxs * mask_ctx[:, :, None]\n     denom = np.maximum(ctx_lens, 1)[:, None]\n     v_context = emb_ctxs.sum(axis=1) / denom           # (B, dim)\n    # gather Huffman paths for targets\n     paths_nodes = paths_nodes_full[targets]            # (B, max_path)\n     paths_bits  = paths_bits_full[targets]             # (B, max_path)\n     max_path = paths_nodes.shape[1]\n     internal_accum = np.zeros_like(self.W_internal, dtype=np.float32)\n     flat_ctx_idx_list = []\n     flat_ctx_delta_list = []\n     total_loss = 0.0\n     CLIP = 30.0   # safe clipping of scores\n     sigmoid = self.sigmoid\n     for p in range(max_path):\n        node_idxs = paths_nodes[:, p]           # (B,)\n        valid_mask = node_idxs != -1\n        if not np.any(valid_mask):\n            continue\n        valid_nodes = node_idxs[valid_mask]     # (Nb,)\n        vctx_valid = v_context[valid_mask]      # (Nb, dim)\n        bits_valid = paths_bits[valid_mask, p]  # (Nb,)\n        node_vecs = self.W_internal[valid_nodes].astype(np.float32)  # (Nb, dim)\n        # scores and clipping (avoid huge exponentials)\n        scores = np.einsum('nd,nd->n', node_vecs, vctx_valid)\n        scores = np.clip(scores, -CLIP, CLIP)\n        # stable loss via logaddexp (no log(sigmoid) directly)\n        pos_mask = bits_valid == 1\n        if np.any(pos_mask):\n            total_loss += np.sum(np.logaddexp(0.0, -scores[pos_mask]))\n        if np.any(~pos_mask):\n            total_loss += np.sum(np.logaddexp(0.0, scores[~pos_mask]))\n        # gradient wrt score using stable sigmoid\n        with np.errstate(over='ignore', invalid='ignore'):\n            sig = sigmoid(scores)   # use your stable sigmoid\n        grad_score = np.where(pos_mask, sig - 1.0, sig)  # (Nb,)\n        # node gradient and grouped accumulation\n        node_grad = (grad_score[:, None] * vctx_valid).astype(np.float32)  # (Nb, dim)\n        #I'm accumulating the gradients. I'm using `np.unique` and `np.bincount` to efficiently\n        # sum the gradients for each unique node ID. It's a bit more complex but very fast.\n        uniq_nodes, inv = np.unique(valid_nodes, return_inverse=True)\n        # sum node_grad rows per unique node id using bincount per dimension\n        summed = np.zeros((uniq_nodes.shape[0], dim), dtype=np.float32)\n        for d in range(dim):\n            summed[:, d] = np.bincount(inv, weights=node_grad[:, d], minlength=uniq_nodes.shape[0])\n        internal_accum[uniq_nodes] += (- self.lr * summed)\n        # backprop to v_context, then distribute to contexts\n        vctx_grad_contrib = (grad_score[:, None] * node_vecs).astype(np.float32)  # (Nb, dim)     # Backpropagate gradient to the context words\n        valid_batch_positions = np.nonzero(valid_mask)[0]\n        for i_local, batch_pos in enumerate(valid_batch_positions):\n            cnt = int(ctx_lens[batch_pos])\n            if cnt == 0:\n                continue\n            grad_v = vctx_grad_contrib[i_local]           # (dim,)\n            per_ctx = (grad_v / cnt).astype(np.float32)\n            ctx_idxs = contexts_padded[batch_pos, :cnt]   # actual indices\n            flat_ctx_idx_list.append(ctx_idxs)\n            flat_ctx_delta_list.append(np.broadcast_to(per_ctx, (ctx_idxs.size, dim)).copy())\n    # apply internal updates once\n     if internal_accum.size:\n        self.W_internal = (self.W_internal.astype(np.float32) + internal_accum).astype(np.float32)\n    # aggregate input updates\n     if flat_ctx_idx_list:\n        flat_idx = np.concatenate(flat_ctx_idx_list).astype(np.int32)\n        flat_deltas = np.concatenate(flat_ctx_delta_list, axis=0).astype(np.float32)  # (M, dim)\n        uniq_idx, inv2 = np.unique(flat_idx, return_inverse=True)\n        summed_in = np.zeros((uniq_idx.shape[0], dim), dtype=np.float32)\n        for d in range(dim):\n            summed_in[:, d] = np.bincount(inv2, weights=flat_deltas[:, d], minlength=uniq_idx.shape[0])\n        # apply update (per_ctx did not include lr)\n        self.W_in[uniq_idx] += (- self.lr * summed_in)\n     avg_loss = float(total_loss) / max(1, B)\n     return avg_loss\n    \n    def train_cbow_ns_batch(self, batch_examples, neg_sampler, K):\n     total_loss = 0.0 # for NS I chose to prepare batches as they come.\n                      # It's slightly less efficient than pre-padding everything, but makes the\n                      # training loop simpler as we don't need to pass around giant arrays.\n     B = len(batch_examples)\n     dim = self.W_in.shape[1]\n    # padded contexts\n     max_ctx = max(len(ctx) for ctx, _ in batch_examples)\n     contexts_padded = -np.ones((B, max_ctx), dtype=np.int32)\n     ctx_lens = np.zeros(B, dtype=np.int32)\n     targets = np.zeros(B, dtype=np.int32)\n     for i, (ctx, tgt) in enumerate(batch_examples):\n        ctx_lens[i] = len(ctx)\n        contexts_padded[i, :len(ctx)] = ctx\n        targets[i] = tgt\n   # Mask to ignore padded positions in embeddings\n     mask_ctx = contexts_padded != -1\n     contexts_safe = np.maximum(0, contexts_padded)\n     emb_ctxs = self.W_in[contexts_safe]\n# Zero-out the embeddings corresponding to padded positions (-1)\n     emb_ctxs = emb_ctxs * mask_ctx[:, :, None]\n     denom = np.maximum(ctx_lens[:, None], 1)   # Compute average context embedding for CBOW\n     v_context = emb_ctxs.sum(axis=1) / denom\n    # Positive target\n     u_target = self.W_out[targets]\n     pos_scores = np.sum(u_target*v_context, axis=1)\n     pos_sig = self.sigmoid(pos_scores)\n     total_loss += -np.sum(np.log(pos_sig+1e-12))\n     grad_pos = pos_sig-1.0\n    # Update positive output embeddings\n     self.W_out[targets] -= self.lr*(grad_pos[:, None] * v_context)\n    # Negative sampling\n     neg_samples = neg_sampler.sample(B*K).reshape(B, K)\n     u_negs = self.W_out[neg_samples]\n     neg_scores = np.einsum('bkd,bd->bk', u_negs, v_context)\n     neg_sig = self.sigmoid(-neg_scores)\n     total_loss += -np.sum(np.log(neg_sig+1e-12)) # for numerical stability \n     grad_neg = 1.0-neg_sig\n    # Update negative embeddings\n     grad_neg_expanded = grad_neg[:, :, None]*v_context[:, None, :]\n     np.add.at(self.W_out, neg_samples.flatten(), -self.lr*grad_neg_expanded.reshape(-1, dim))\n     # The actual gradient w.r.t v_context\n     grad_from_pos = grad_pos[:, None] * u_target\n     grad_from_negs = np.sum((1-grad_neg)[:, :, None]*u_negs, axis=1)\n     grad_input = grad_from_pos+grad_from_negs\n    # Distribute gradient to each context word correctly\n     # for i in range(B):\n     #    ctx = contexts_padded[i, :ctx_lens[i]]\n     #    if len(ctx) > 0:\n     #        self.W_in[ctx] -= self.lr * (grad_input[i] / ctx_lens[i])\n     clip_threshold = 5.0    # Gradient clipping for stability \n    # Calculate the L2 norm of the gradient matrix\n     norm = np.linalg.norm(grad_input, axis=1, keepdims=True)\n    # If the norm exceeds the threshold, scale it down\n    # We add 1e-8 for numerical stability in case norm is zero\n     scale = np.minimum(1.0, clip_threshold/(norm+1e-8))\n     grad_input = grad_input*scale\n     grad_dist = grad_input/denom     # Distribute gradient evenly across context words\n     grad_to_apply = grad_dist[np.arange(B).repeat(ctx_lens)]\n     valid_ctx_indices = contexts_padded[mask_ctx]\n     np.add.at(self.W_in, valid_ctx_indices, -self.lr*grad_to_apply) #np.add.at ensures correct accumulation if context words repeat in the batch.\n     avg_loss = total_loss/max(1, B)\n     return avg_loss","metadata":{"id":"xrZ7_hTX0gnC","trusted":true,"execution":{"iopub.status.busy":"2025-09-30T12:44:04.102273Z","iopub.execute_input":"2025-09-30T12:44:04.102505Z","iopub.status.idle":"2025-09-30T12:44:04.537106Z","shell.execute_reply.started":"2025-09-30T12:44:04.102486Z","shell.execute_reply":"2025-09-30T12:44:04.536451Z"}},"outputs":[],"execution_count":8},{"cell_type":"markdown","source":"**CBOW+HS**","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport time\nimport math\nimport random\nimport os\nimport pickle\nLR = 0.025\nBATCH_SIZE = 128\nEPOCHS = 2\nEMBEDDING_DIM = 100\nall_contexts = [ctx for ctx, _ in cbow_examples]\nall_targets = np.array([tgt for _, tgt in cbow_examples], dtype=np.int32)\nall_paths = [huff_paths[tgt] for tgt in all_targets]\nall_codes = [huff_codes[tgt] for tgt in all_targets]\nN = len(cbow_examples)\nmax_ctx_len = max(len(ctx) for ctx in all_contexts) if all_contexts else 0\nmax_path_len = max(len(p) for p in all_paths) if all_paths else 0\ncontexts_padded_full = -np.ones((N, max_ctx_len), dtype=np.int32)\nctx_lens_full = np.array([len(ctx) for ctx in all_contexts], dtype=np.int32)\npaths_padded_full = -np.ones((N, max_path_len), dtype=np.int32)\ncodes_padded_full = -np.ones((N, max_path_len), dtype=np.int32)\npath_lens_full = np.array([len(p) for p in all_paths], dtype=np.int32)\nrow_idx_ctx = np.repeat(np.arange(N), ctx_lens_full)\ncol_idx_ctx = np.concatenate([np.arange(l) for l in ctx_lens_full])\nif row_idx_ctx.size > 0: # Check if there is any context data to place\n    contexts_padded_full[row_idx_ctx, col_idx_ctx] = np.concatenate(all_contexts)\nrow_idx_path = np.repeat(np.arange(N), path_lens_full)\ncol_idx_path = np.concatenate([np.arange(l) for l in path_lens_full])\nif row_idx_path.size > 0: # Check if there is any path data to place\n    paths_padded_full[row_idx_path, col_idx_path] = np.concatenate(all_paths)\n    codes_padded_full[row_idx_path, col_idx_path] = np.concatenate(all_codes)\nmodels = {}\nmodes = [(\"cbow\", \"hs\")]\nfor method, loss in modes:\n    print(f\"\\nStarting training: {method} + {loss}\")\n    model = Word2Vec(vocab, dim=EMBEDDING_DIM, lr=LR)\n    if loss == \"hs\":\n        model.init_hs(huff_codes, huff_paths, huff_n_internal)\n    n_batches = math.ceil(N / BATCH_SIZE)\n    initial_lr = LR\n    end_lr = 0.0001\n    total_steps = n_batches * EPOCHS\n    current_step = 0\n    try:\n        for ep in range(1, EPOCHS + 1):\n            indices = np.arange(N)\n            np.random.shuffle(indices)\n            epoch_loss = 0.0\n            start_time = time.time()\n            for batch_start in range(0, N, BATCH_SIZE):\n                # Update learning rate\n                progress = current_step / total_steps\n                model.lr = max(initial_lr - (initial_lr - end_lr) * progress, end_lr)\n                current_step += 1\n                # Fast slicing of pre-processed NumPy arrays\n                batch_idx = indices[batch_start : batch_start + BATCH_SIZE]\n                if method == \"cbow\" and loss == \"hs\":\n                    # Pass the pre-computed, sliced arrays directly to the training function\n                    batch_loss = model.train_cbow_hs_batch(\n                        batch_idx,\n                        contexts_padded_full,\n                        ctx_lens_full,\n                        all_targets,\n                        paths_padded_full,\n                        codes_padded_full,\n                        path_lens_full\n                    )\n                else:\n                    batch_loss = 0\n                epoch_loss += float(batch_loss)\n            avg_loss = epoch_loss / n_batches\n            elapsed = time.time() - start_time\n            print(f\"Epoch {ep}/{EPOCHS} finished — avg_loss: {avg_loss:.4f} — current_lr: {model.lr:.6f} — time: {elapsed:.1f}s\")\n    except (Exception, KeyboardInterrupt) as e:\n        print(f\"\\nTraining interrupted at epoch {ep if 'ep' in locals() else 0}. Error: {e}\")\n        print(\"Saving current model state...\")\n    finally:\n        emb_fname = f\"{method}_{loss}.pkl\"\n        out_path = os.path.join(OUT_ROOT, emb_fname)\n        if 'model' in locals():\n            with open(out_path, \"wb\") as f:\n                pickle.dump({\n                    \"embeddings\": model.W_in.astype(np.float32),\n                    \"vocab\": vocab.idx2word,\n                    \"stoi\": vocab.word2idx\n                }, f)\n            print(\"Final model saved:\", out_path)\n            models[f\"{method}_{loss}\"] = model","metadata":{"id":"tp22CCaW0gsx","trusted":true,"execution":{"iopub.status.busy":"2025-09-30T12:49:52.811993Z","iopub.execute_input":"2025-09-30T12:49:52.812719Z","iopub.status.idle":"2025-09-30T14:48:47.263633Z","shell.execute_reply.started":"2025-09-30T12:49:52.812695Z","shell.execute_reply":"2025-09-30T14:48:47.262787Z"}},"outputs":[{"name":"stdout","text":"Pre-processing CBOW data into padded NumPy arrays (fully vectorized)...\nPre-processing complete.\n\nStarting training: cbow + hs\nInitializing Hierarchical Softmax components...\nEpoch 1/2 finished — avg_loss: 4.4988 — current_lr: 0.012550 — time: 3472.9s\nEpoch 2/2 finished — avg_loss: 4.2891 — current_lr: 0.000100 — time: 3504.3s\nFinal model saved: Task_1/cbow_hs_last.pkl\n","output_type":"stream"}],"execution_count":12},{"cell_type":"markdown","source":"**SKIPGRAM+NS**","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport time\nimport math\nimport random\nimport os\nimport pickle\nLR = 0.025 #Word2vec paper uses this with linear decay\nBATCH_SIZE = 128\nEPOCHS = 5\nEMBEDDING_DIM = 100\nNEGATIVE_SAMPLES = 5 # Standard value for negative sampling\nsg_pairs_np = np.array(sg_pairs, dtype=np.int32)\ncenter_words = sg_pairs_np[:, 0]\ncontext_words = sg_pairs_np[:, 1]\nmodels = {}\nmodes = [(\"skipgram\", \"ns\")]\nfor method, loss in modes:\n    print(f\"\\nStarting training: {method} + {loss}\")\n    model = Word2Vec(vocab, dim=EMBEDDING_DIM, lr=LR)\n    N = len(center_words)\n    n_batches = math.ceil(N / BATCH_SIZE)\n    initial_lr = LR\n    end_lr = 0.0001\n    total_steps = n_batches * EPOCHS\n    current_step = 0\n    try:\n        for ep in range(1, EPOCHS + 1):\n            indices = np.arange(N) # Use np.arange for NumPy arrays\n            np.random.shuffle(indices)\n            epoch_loss = 0.0\n            start_time = time.time()\n            for batch_start in range(0, N, BATCH_SIZE):\n                # --- Update LR for current step ---\n                progress = current_step / total_steps\n                model.lr = max(initial_lr-(initial_lr - end_lr) * progress, end_lr)\n                current_step += 1\n                batch_idx = indices[batch_start : batch_start + BATCH_SIZE]\n                center_batch = center_words[batch_idx]\n                context_batch = context_words[batch_idx]\n                if method == \"skipgram\" and loss == \"ns\":\n                    batch_loss = model.train_skipgram_ns_batch(\n                        center_batch,\n                        context_batch,\n                        neg_sampler,\n                        NEGATIVE_SAMPLES\n                    )\n                else:\n  #placeholder\n                    batch_loss = 0\n                epoch_loss += float(batch_loss)\n            avg_loss = epoch_loss / n_batches\n            elapsed = time.time()-start_time\n            print(f\"Epoch {ep}/{EPOCHS} finished — avg_loss: {avg_loss:.4f} — current_lr: {model.lr:.6f} — time: {elapsed:.1f}s\")\n            checkpoint_fname = f\"{method}_{loss}_epoch_{ep}.pkl\"\n            checkpoint_path = os.path.join(OUT_ROOT, checkpoint_fname)\n            print(f\"Saving checkpoint to {checkpoint_path}...\")\n            with open(checkpoint_path, \"wb\") as f:\n                pickle.dump({\n                    \"embeddings\": model.W_in.astype(np.float32),\n                    \"vocab\": vocab.idx2word,\n                    \"stoi\": vocab.word2idx\n                }, f)\n    except Exception as e:\n        print(f\"\\nTraining interrupted at epoch {ep if 'ep' in locals() else 0}. Error: {e}\")\n        print(\"Saving current model state...\")\n    finally:\n        emb_fname = f\"{method}_{loss}.pkl\"\n        out_path = os.path.join(OUT_ROOT, emb_fname)\n        with open(out_path, \"wb\") as f:\n            if 'model' in locals():\n                pickle.dump({\n                    \"embeddings\": model.W_in.astype(np.float32),\n                    \"vocab\": vocab.idx2word,\n                    \"stoi\": vocab.word2idx\n                }, f)\n        print(\"Final model saved:\", out_path)\n        models[f\"{method}_{loss}\"] = model","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"I trained skipgram with negative sampling in Google Colab. I have pasted the log here.\n\nStarting training: skipgram + ns\n\nEpoch 1/5 finished — avg_loss: 2.2572 — current_lr: 0.020020 — time: 851.6s\n\nSaving checkpoint to Task_1/skipgram_ns_epoch_1.pkl\n\nEpoch 2/5 finished — avg_loss: 2.1070 — current_lr: 0.015040 — time: 843.3s\n\nSaving checkpoint to Task_1/skipgram_ns_epoch_2.pkl\n\nEpoch 3/5 finished — avg_loss: 2.0662 — current_lr: 0.010060 — time: 847.7s\n\nSaving checkpoint to Task_1/skipgram_ns_epoch_3.pkl\n\nEpoch 4/5 finished — avg_loss: 2.0384 — current_lr: 0.005080 — time: 836.6s\n\nSaving checkpoint to Task_1/skipgram_ns_epoch_4.pkl\n\nEpoch 5/5 finished — avg_loss: 2.0164 — current_lr: 0.000100 — time: 830.0s\n\nSaving checkpoint to Task_1/skipgram_ns_epoch_5.pkl\n\nFinal model saved: Task_1/skipgram_ns_last.pkl","metadata":{}},{"cell_type":"markdown","source":"**CBOW+NS**","metadata":{}},{"cell_type":"code","source":"LR = 0.025\nBATCH_SIZE = 128\nEPOCHS = 5\nEMBEDDING_DIM = 100 \nNEGATIVE_SAMPLES = 5 \nimport math\nimport random\nimport os\nimport pickle\nimport numpy as np\nmodels = {}\nmodes = [(\"cbow\", \"ns\")]\nfor method, loss in modes:\n    print(f\"\\nStarting training: {method} + {loss}\")\n    model = Word2Vec(vocab, dim=EMBEDDING_DIM, lr=LR)\n    if loss == \"hs\":\n        pass\n    data = sg_pairs if method == \"skipgram\" else cbow_examples\n    N = len(data)\n    n_batches = math.ceil(N / BATCH_SIZE)\n    initial_lr = LR  # The starting learning rate \n    end_lr = 0.0001  # The final learning rate (a small non-zero value is common)\n    total_steps = n_batches * EPOCHS  # Total number of batches to be processed\n    current_step = 0  # Initialize a counter for batches\n    try:\n        for ep in range(1, EPOCHS + 1):\n            indices = list(range(N))\n            random.shuffle(indices)\n            epoch_loss = 0.0\n            start_time = time.time()\n            for batch_start in range(0, N, BATCH_SIZE):\n                progress = current_step / total_steps\n                # Linearly interpolate the LR between the start and end values\n                new_lr = initial_lr - (initial_lr - end_lr) * progress\n                # Updating the model's learning rate for the upcoming batch\n                # Use max() to avoid errors in floating point\n                model.lr = max(new_lr, end_lr)\n                current_step += 1\n                batch_idx = indices[batch_start: batch_start + BATCH_SIZE]\n                if method == \"skipgram\" and loss == \"ns\":\n                    pass\n                elif method == \"cbow\" and loss == \"ns\":\n                    batch_data = [data[i] for i in batch_idx]\n                    batch_loss = model.train_cbow_ns_batch(batch_data, neg_sampler, NEGATIVE_SAMPLES)\n                else:\n                    batch_loss = 0 # Placeholder for other methods\n                epoch_loss += float(batch_loss)\n            avg_loss = epoch_loss / n_batches\n            elapsed = time.time() - start_time\n            print(f\"Epoch {ep}/{EPOCHS} finished — avg_loss: {avg_loss:.4f} — current_lr: {model.lr:.6f} — time: {elapsed:.1f}s\")\n    except Exception as e:\n        print(f\"\\nTraining interrupted at epoch {ep}. Error: {e}\")\n        print(\"Saving current model state...\")\n    finally:\n        emb_fname = f\"{method}_{loss}.pkl\"\n        out_path = os.path.join(OUT_ROOT, emb_fname)\n        with open(out_path, \"wb\") as f:\n            pickle.dump({\n                \"embeddings\": model.W_in.astype(np.float32),\n                \"vocab\": vocab.idx2word,\n                \"stoi\": vocab.word2idx\n            }, f)\n        print(\"Model saved:\", out_path)\n        models[f\"{method}_{loss}\"] = model","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-30T07:53:53.515538Z","iopub.execute_input":"2025-09-30T07:53:53.516070Z","iopub.status.idle":"2025-09-30T08:21:30.385315Z","shell.execute_reply.started":"2025-09-30T07:53:53.516042Z","shell.execute_reply":"2025-09-30T08:21:30.384526Z"}},"outputs":[{"name":"stdout","text":"\nStarting training: cbow + ns\nEpoch 1/5 finished — avg_loss: 39.0056 — current_lr: 0.020020 — time: 326.8s\nEpoch 2/5 finished — avg_loss: 38.6389 — current_lr: 0.015040 — time: 324.4s\nEpoch 3/5 finished — avg_loss: 37.9681 — current_lr: 0.010060 — time: 324.1s\nEpoch 4/5 finished — avg_loss: 37.4282 — current_lr: 0.005080 — time: 324.0s\nEpoch 5/5 finished — avg_loss: 36.4011 — current_lr: 0.000100 — time: 324.4s\nModel saved: Task_1/cbow_ns_last.pkl\n","output_type":"stream"}],"execution_count":43},{"cell_type":"markdown","source":"**SKIPGRAM+HS**","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport time\nimport math\nimport random\nimport os\nimport pickle\nLR = 0.025\nBATCH_SIZE = 128\nEPOCHS = 1\nEMBEDDING_DIM = 100\nsg_pairs_np = np.array(sg_pairs, dtype=np.int32)\ncenter_words = sg_pairs_np[:, 0]\ncontext_words = sg_pairs_np[:, 1]\nmodels = {}\nmodes = [(\"skipgram\", \"hs\")]\nfor method, loss in modes:\n    print(f\"\\nStarting training: {method} + {loss}\")\n    model = Word2Vec(vocab, dim=EMBEDDING_DIM, lr=LR)\n    if loss == \"hs\":\n        print(\"Initializing Hierarchical Softmax components...\")\n        model.init_hs(huff_codes, huff_paths, huff_n_internal)\n    N = len(center_words)\n    n_batches = math.ceil(N / BATCH_SIZE)\n    initial_lr = LR\n    end_lr = 0.0001\n    total_steps = n_batches * EPOCHS\n    current_step = 0\n    try:\n        for ep in range(1, EPOCHS + 1):\n            indices = np.arange(N) # Use np.arange for NumPy arrays\n            np.random.shuffle(indices)\n            epoch_loss = 0.0\n            start_time = time.time()\n            for batch_start in range(0, N, BATCH_SIZE):\n                # updating LR\n                progress = current_step / total_steps\n                model.lr = max(initial_lr - (initial_lr - end_lr) * progress, end_lr)\n                current_step += 1\n                batch_idx = indices[batch_start : batch_start + BATCH_SIZE]\n                center_batch = center_words[batch_idx]\n                context_batch = context_words[batch_idx]\n                batch_pairs = list(zip(center_batch, context_batch))\n                if method == \"skipgram\" and loss == \"hs\":\n                    batch_loss = model.train_skipgram_hs_batch(batch_pairs) \n                else:\n                    batch_loss = 0\n                epoch_loss += float(batch_loss)\n            avg_loss = epoch_loss / n_batches\n            elapsed = time.time() - start_time\n            print(f\"Epoch {ep}/{EPOCHS} finished — avg_loss: {avg_loss:.4f} — current_lr: {model.lr:.6f} — time: {elapsed:.1f}s\")\n            checkpoint_fname = f\"{method}_{loss}_epoch_{ep}.pkl\"\n            checkpoint_path = os.path.join(OUT_ROOT, checkpoint_fname)\n            print(f\"Saving checkpoint to {checkpoint_path}...\")\n            with open(checkpoint_path, \"wb\") as f:\n                pickle.dump({\n                    \"embeddings\": model.W_in.astype(np.float32),\n                    \"vocab\": vocab.idx2word,\n                    \"stoi\": vocab.word2idx\n                }, f)\n    except Exception as e:\n        print(f\"\\nTraining interrupted at epoch {ep if 'ep' in locals() else 0}. Error: {e}\")\n        print(\"Saving current model state...\")\n    finally:\n        emb_fname = f\"{method}_{loss}.pkl\"\n        out_path = os.path.join(OUT_ROOT, emb_fname)\n        with open(out_path, \"wb\") as f:\n            if 'model' in locals():\n                pickle.dump({\n                    \"embeddings\": model.W_in.astype(np.float32),\n                    \"vocab\": vocab.idx2word,\n                    \"stoi\": vocab.word2idx\n                }, f)\n        print(\"Final model saved:\", out_path)\n        models[f\"{method}_{loss}\"] = model","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-30T10:18:32.048529Z","iopub.execute_input":"2025-09-30T10:18:32.048797Z","iopub.status.idle":"2025-09-30T11:57:13.390517Z","shell.execute_reply.started":"2025-09-30T10:18:32.048778Z","shell.execute_reply":"2025-09-30T11:57:13.389642Z"}},"outputs":[{"name":"stdout","text":"Pre-processing data into NumPy arrays for fast slicing...\nPre-processing complete.\n\nStarting training: skipgram + hs\nInitializing Hierarchical Softmax components...\nEpoch 1/1 finished — avg_loss: 8.6854 — current_lr: 0.000100 — time: 5904.7s\nSaving checkpoint to Task_1/skipgram_hs_epoch_1.pkl...\nFinal model saved: Task_1/skipgram_hs_last.pkl\n","output_type":"stream"}],"execution_count":69},{"cell_type":"code","source":"import os\nimport pickle\nimport numpy as np\nimport pandas as pd\nfrom datasets import load_dataset\nimport sys\nMODEL_DIR = \"/kaggle/input/word2vec/other/default/1\"\nMODEL_NAMES = {\n    \"skipgram_ns_word\": \"skipgram_ns.pkl\",\n    \"skipgram_hs_word\": \"skipgram_hs.pkl\",\n    \"cbow_ns_word\":     \"cbow_ns.pkl\",\n    \"cbow_hs_word\":     \"cbow_hs.pkl\"\n}\nOUT_CSV = \"word2vec_analogy_results.csv\"\ndef load_model(path):\n    with open(path, \"rb\") as f:\n        data = pickle.load(f)\n    emb = np.asarray(data[\"embeddings\"], dtype=np.float32)\n    # Normalize rows (unit vectors) to use dot product as cosine similarity\n    norms = np.linalg.norm(emb, axis=1, keepdims=True)\n    emb = emb / (norms + 1e-12)\n    return emb, list(data[\"vocab\"]), dict(data[\"stoi\"])\ndef get_idx(token, stoi):\n    return stoi.get(token, stoi.get(\"<unk>\"))\n\ndef predict_one(model, w1, w2, w3):\n    emb, vocab, stoi = model[\"emb\"], model[\"vocab\"], model[\"stoi\"]\n    i1, i2, i3 = get_idx(w1, stoi), get_idx(w2, stoi), get_idx(w3, stoi)\n    if any(i is None for i in [i1, i2, i3]):\n        return \"OOV\" # Out-of-Vocabulary\n    vec = emb[i1] + emb[i2] - emb[i3]\n    vec_norm = np.linalg.norm(vec)\n    if vec_norm < 1e-9:\n        return \"OOV\"\n    vec /= vec_norm\n    sims = emb.dot(vec)\n    # Exclude the three input words from theanswers\n    sims[[i1, i2, i3]] = -np.inf\n    # Exclude common special tokens if they exist in the vocabulary\n    for special_token in (\"<pad>\", \"<unk>\", \"<s>\", \"</s>\"):\n        if special_token in stoi:\n            sims[stoi[special_token]] = -np.inf\n    best_idx = int(np.argmax(sims))\n    return vocab[best_idx]\nmodels = {}\nfor key, fname in MODEL_NAMES.items():\n    path = os.path.join(MODEL_DIR, fname)\n    models[key] = {\"emb\": None, \"vocab\": None, \"stoi\": None}\n    models[key][\"emb\"], models[key][\"vocab\"], models[key][\"stoi\"] = load_model(path)\nds = load_dataset(\"Exploration-Lab/CS779-Fall25\", \"Assignment-3-word2vec-analogy\", split=\"test\", token=hf_token, trust_remote_code=True)\nexamples = []\nexpected_cols = ['word1', 'word2', 'word3']\nfor row in ds:\n            # The operators are implied by the predict_one function.\n            # We add them here to match the structure of the prediction loop.\n            examples.append((row['word1'], '+', row['word2'], '-', row['word3']))\nrows = []\nfor (w1, op1, w2, op2, w3) in examples:\n    row = {\"word1\": w1, \"op1\": op1, \"word2\": w2, \"op2\": op2, \"word3\": w3}\n    for model_key in models.keys():\n        row[model_key] = predict_one(models[model_key], w1, w2, w3)\n    rows.append(row)\ndf_columns = [\"word1\", \"op1\", \"word2\", \"op2\", \"word3\"] + list(models.keys())\ndf = pd.DataFrame(rows, columns=df_columns)\ndf.to_csv(OUT_CSV, index=False)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-30T16:03:50.733334Z","iopub.execute_input":"2025-09-30T16:03:50.733694Z","iopub.status.idle":"2025-09-30T16:03:56.789955Z","shell.execute_reply.started":"2025-09-30T16:03:50.733645Z","shell.execute_reply":"2025-09-30T16:03:56.789040Z"}},"outputs":[],"execution_count":21},{"cell_type":"code","source":"import pickle\nimport numpy as np\nimport random\nimport os\nfrom sklearn.manifold import TSNE\nimport umap\nimport plotly.express as px\n\nMODELS_TO_VISUALIZE = {\n    \"cbow_ns\": \"/kaggle/input/word2vec/other/default/1/cbow_ns.pkl\",\n    \"cbow_hs\": \"/kaggle/input/word2vec/other/default/1/cbow_hs.pkl\",\n    \"skipgram_ns\": \"/kaggle/input/word2vec/other/default/1/skipgram_ns.pkl\",\n    \"skipgram_hs\": \"/kaggle/input/word2vec/other/default/1/skipgram_hs.pkl\"\n}\n\nNUM_SEED_WORDS = 50\nNUM_NEIGHBORS = 10 # Top N most similar words to find for each seed word\ndef find_nearest_neighbors(word_idx, embeddings_norm, k):\n    # normalized vector for the query word\n    query_vector =embeddings_norm[word_idx]\n    # cosine similarity (dot product of normalized vectors)\n    similarities =np.dot(embeddings_norm, query_vector)\n    #indices of the top k+1 most similar words (includes the word itself)\n    top_indices= np.argsort(similarities)[::-1][1:k+1] \n    return top_indices\n\ndef generate_plot(coords, words, groups, model_name, reduction_method):\n    fig = px.scatter(\n        x=coords[:, 0],\n        y=coords[:, 1],\n        hover_name=words,\n        color=groups,  \n        title=f\"{reduction_method} Visualization | {model_name}\"\n    )\n    fig.update_traces(\n        marker=dict(size=8, opacity=0.8),\n        selector=dict(mode='markers')\n    )\n    fig.update_layout(\n        xaxis_title=f\"{reduction_method} 1\",\n        yaxis_title=f\"{reduction_method} 2\",\n        legend_title=\"Seed Word\"\n    )\n    file_prefix = model_name.lower().replace(' + ', '_').replace(' ', '_')\n    filename = f\"{file_prefix}_{reduction_method.lower()}.html\"\n    fig.write_html(filename)\n    print(f\"    - Saved plot to {filename}\")\ndef main():\n    for model_name, model_path in MODELS_TO_VISUALIZE.items():\n        print(f\"\\n--- Processing Model: {model_name} ---\")\n        with open(model_path, 'rb') as f:\n            data = pickle.load(f)\n        embeddings = data['embeddings']\n        idx2word = data['vocab']\n        word2idx = data['stoi']\n        #  normalize the entire embedding matrix fro efficiency\n        embeddings_norm = embeddings / np.linalg.norm(embeddings, axis=1, keepdims=True)\n        # 2. Randomly sample 50 words\n        print(f\"  - Randomly sampling {NUM_SEED_WORDS} seed words...\")\n        valid_words = [word for word in word2idx if word not in ('<unk>', '<pad>')]\n        seed_words = random.sample(valid_words, NUM_SEED_WORDS)\n        # 3. Find top neighbors and gather data for plotting\n        print(f\"  - Finding top {NUM_NEIGHBORS} neighbors for each seed word...\")\n        words_for_plot = []\n        vectors_for_plot = []\n        group_labels = [] # To color clusters in the plot\n        for seed_word in seed_words:\n            seed_idx = word2idx[seed_word]\n            neighbor_indices = find_nearest_neighbors(seed_idx, embeddings_norm, k=NUM_NEIGHBORS)\n            # Combine the seed word and its neighbors\n            cluster_indices = [seed_idx] + list(neighbor_indices) \n            # Add the data for this cluster to our lists\n            for idx in cluster_indices:\n                words_for_plot.append(idx2word[idx])\n                vectors_for_plot.append(embeddings[idx])\n                group_labels.append(seed_word) # Use the seed word as the group label\n        embedding_matrix_for_reduction = np.array(vectors_for_plot)\n        total_words = len(words_for_plot)\n        tsne = TSNE(n_components=2, random_state=42, perplexity=min(30, total_words - 1))\n        coords_tsne = tsne.fit_transform(embedding_matrix_for_reduction)\n        umapper = umap.UMAP(n_components=2, random_state=42, n_neighbors=min(15, total_words - 1))\n        coords_umap = umapper.fit_transform(embedding_matrix_for_reduction)\n        generate_plot(coords_tsne, words_for_plot, group_labels, model_name, \"tsne\")\n        generate_plot(coords_umap, words_for_plot, group_labels, model_name, \"umap\")\nif __name__ == '__main__':\n    main()\n","metadata":{"id":"Wb-GSJvr0hBn","trusted":true,"execution":{"iopub.status.busy":"2025-09-30T15:36:03.164054Z","iopub.execute_input":"2025-09-30T15:36:03.164363Z","iopub.status.idle":"2025-09-30T15:36:20.309982Z","shell.execute_reply.started":"2025-09-30T15:36:03.164338Z","shell.execute_reply":"2025-09-30T15:36:20.308965Z"}},"outputs":[{"name":"stdout","text":"\n--- Processing Model: cbow_ns ---\n  - Randomly sampling 50 seed words...\n  - Finding top 10 neighbors for each seed word...\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/umap/umap_.py:1952: UserWarning:\n\nn_jobs value 1 overridden to 1 by setting random_state. Use no seed for parallelism.\n\n","output_type":"stream"},{"name":"stdout","text":"    - Saved plot to cbow_ns_tsne.html\n    - Saved plot to cbow_ns_umap.html\n\n--- Processing Model: cbow_hs ---\n  - Randomly sampling 50 seed words...\n  - Finding top 10 neighbors for each seed word...\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/umap/umap_.py:1952: UserWarning:\n\nn_jobs value 1 overridden to 1 by setting random_state. Use no seed for parallelism.\n\n","output_type":"stream"},{"name":"stdout","text":"    - Saved plot to cbow_hs_tsne.html\n    - Saved plot to cbow_hs_umap.html\n\n--- Processing Model: skipgram_ns ---\n  - Randomly sampling 50 seed words...\n  - Finding top 10 neighbors for each seed word...\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/umap/umap_.py:1952: UserWarning:\n\nn_jobs value 1 overridden to 1 by setting random_state. Use no seed for parallelism.\n\n","output_type":"stream"},{"name":"stdout","text":"    - Saved plot to skipgram_ns_tsne.html\n    - Saved plot to skipgram_ns_umap.html\n\n--- Processing Model: skipgram_hs ---\n  - Randomly sampling 50 seed words...\n  - Finding top 10 neighbors for each seed word...\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/umap/umap_.py:1952: UserWarning:\n\nn_jobs value 1 overridden to 1 by setting random_state. Use no seed for parallelism.\n\n","output_type":"stream"},{"name":"stdout","text":"    - Saved plot to skipgram_hs_tsne.html\n    - Saved plot to skipgram_hs_umap.html\n","output_type":"stream"}],"execution_count":4},{"cell_type":"markdown","source":"References:\n\nhttps://medium.com/@gsoykan/training-word2vec-skip-gram-with-hierarchical-softmax-in-pytorch-bbd3363bcff9\n\nhttps://colab.research.google.com/github/nickvdw/word2vec-from-scratch/blob/master/word2vec.ipynb\n\nhttps://towardsdatascience.com/word2vec-research-paper-explained-205cb7eecc30/","metadata":{}},{"cell_type":"markdown","source":"## **3. Analytical & Critical Thinking Questions [$6 \\times 20 = 120$ marks]**\n\n---\n\n### **Q1. Skip-gram vs CBOW Trade-offs**\nGiven a highly imbalanced corpus where rare words appear very few times and common words dominate, which architecture (Skip-gram or CBOW) would you prefer and why?  \nJustify your answer using the mathematical objectives and data characteristics.\n\n**A.** In a highly imbalanced corpus, Skip-gram should be preferred over CBOW. The reason is that how each model treats rare versus frequent words:\n\n**Differences:**\n\n* CBOW predicts a target word from its context **(averaged embeddings)**. This averaging means that rare words—appearing in few contexts, will contribute very little to the overall gradient. As a result, CBOW embeddings for rare words are poorly learned.\n\n* Skip-gram predicts context words from a given **center word**. Each occurrence of a rare word is treated individually, generating multiple context predictions. Mathematically, the loss for each rare word still accumulates over its few occurrences, making the gradient updates more informative.\n\n**Data characteristics:**\n\n* In an imbalanced corpus, frequent words dominate the context windows. CBOW’s averaging causes these frequent words to overshadow rare words, weakening rare word embeddings.\n\n* Skip-gram naturally gives each center word, regardless of frequency, its own set of predictions. This helps the model “pay more attention” to rare words despite the imbalance.\n\n**Observations:**\n\n* Skip-gram produces better embeddings for rare words, while CBOW is faster and works well when the corpus is large and word frequencies are more balanced.\n\n**Conclusion:** Skip-gram is better for imbalanced corpora because its loss formulation preserves characteristics from rare words, resulting in more meaningful embeddings for them, whereas CBOW underrepresents rare word semantics.\n\n\n\n### **Q2. Impact of Negative Sampling Distribution**\nIf negative samples are drawn uniformly at random versus proportionally to word frequency, how would the embeddings differ?  \nWhich strategy do you think better captures semantic relationships, and why?\n\n**A.** The choice of negative sampling distribution affects what the embeddings capture:\n\n**Uniform negative sampling:** With uniform negative sampling, the model picks random words as negatives without considering how often they actually appear in real text. That means rare words show up more often than they should during training. The model ends up wasting energy trying to separate these rare words from contexts they don’t really belong to. This can mess with the way word meanings are learned, making the final embeddings less accurate and less reflective of how words are used in real language.\n\n**Frequency-proportional (power-law) negative sampling:** In frequency-based negative sampling, words are picked based on how often they show up in real text, but with a slight adjustment (raising the frequency to the ¾ power). This means common words are chosen more often, but rare words still get picked sometimes. It helps the model learn to tell a word apart from other words that actually appear nearby in real language, instead of wasting effort on random or unlikely examples. That makes the word meanings it learns more accurate and useful\nSemantic relationships are captured more faithfully because the model learns to differentiate words from the “background noise” of frequent terms.\n\nFor example - Imagine trying to teach a child that “cat” is related to “kitten.” If we repeatedly force the child to distinguish “kitten” from extremely rare words they hardly ever see, the learning is noisy. Instead, asking them to distinguish “kitten” from common words like “the” or “and” helps them focus on meaningful distinctions.\n\n**Conclusion:** When we sample negative words based on how often they actually appear in real text (Frequency-proportional negative sampling), the model learns better because it sees patterns that reflect how language is really used. If we just pick random words equally (uniform sampling), we might end up training on words that rarely show up together, which doesn’t help the model understand meaning. It’s like trying to learn relationships between words that don’t belong together—it wastes effort and makes the results less accurate.\nTherefore, Frequency-proportional negative sampling better captures semantic relationships than uniform sampling.\n\n\n### **Q3. Analogy Task Evaluation**\nExplain why this vector arithmetic works in the embedding space.  \nWhat assumptions about word co-occurrence and semantic regularities does this rely on?  \nCan you think of situations where analogy tasks might fail, even if embeddings are high quality?\n\n**A.** **Why vector arithmetic works in embedding space:**\n\nWord embeddings encode semantic and syntactic information in a continuous vector space. They are like smart number maps for words. When we train models like Skip-gram or CBOW, they learn to place words that show up in similar situations close together in this map. That way, words with similar meanings end up having similar vectors. What’s cool is that this map can also capture relationships—like the difference between “king” and “queen” often looks the same as the difference between “man” and “woman.” So you can actually do math with words and get meaningful results.\n\n* Relationships show up as directions: Some word relationships like gender, tense, or geography tend to follow similar paths in the embedding space. For example, the difference between “king” and “queen” often points in the same direction as “man” to “woman.” These patterns form straight lines that help the model understand how words relate.\n\n* Similar contexts pull words together: Words that appear in similar situations like “dog” and “puppy” end up close to each other in the vector space. At the same time, differences between them (like age or size) show up as consistent shifts in their positions.\n\n* Words learn from their neighbors: The model learns meaning by looking at which words tend to appear together. This idea—called the distributional hypothesis means that a word’s meaning is shaped by the company it keeps. These co-occurrence patterns naturally form structured, almost linear relationships in the embedding space.\n\n**Assumptions underlying this arithmetic:**\n\n* Words need enough examples: For word embeddings to learn meaningful patterns, each word has to appear in enough different contexts. If a word is too rare or the dataset is unbalanced, the model might not learn its relationships properly.\n\n* Meaning adds up like math: Embeddings assume that certain features—like gender or verb tense—can be added or subtracted like numbers. So the difference between “he” and “she” or “walk” and “walked” can be captured as a consistent shift in the vector space.\n\n* Close vectors mean similar ideas: In the embedding space, words that are close together are expected to have similar meanings. It’s a smooth map—small changes in the vector usually reflect small changes in meaning.\n\n**Situations where analogies might fail even with high-quality embeddings:**\n\n* Sparse data for rare words: If one of the words appears rarely, its vector might be noisy.\n\n* Polysemy and context-dependence: Words with multiple meanings (bank, apple) can confuse the arithmetic, since a single vector averages multiple senses.\n\n* Non-linear relations: Some relationships, like hypernymy (dog → animal) or complex syntactic patterns, are not perfectly linear and may not be captured by simple vector subtraction.\n\n* Cultural or domain-specific terms: Analogies requiring outside-world knowledge may fail if training data lacks relevant contexts.\n\n\n### **Q4. Effect of Embedding Dimension**\n\nWhat would happen if we drastically reduce the embedding dimension (e.g., 10) or increase it (e.g., 1000)?  \nDiscuss the trade-offs in terms of representation power, training stability, overfitting, and downstream evaluation tasks like analogy reasoning.\n\n**A.** The embedding dimension controls the capacity of the model to represent semantic and syntactic information.\n\n**Very low dimension (e.g., 10):**\n\n* With very low dimensions, the model doesn’t have enough space to represent all the subtle differences between words. So many words end up squeezed together, even if they don’t mean the same thing.\n\n* Things like tone, context, or slight changes in meaning are hard to capture. The model can’t tell apart words that should be different like “happy” vs “content” or “run” vs “jog.”\n\n* Even if training looks stable, the embeddings won’t be very useful for tasks like finding similar words or solving analogies. The model just doesn’t have enough detail.\n\n* The whole embedding space becomes coarse and blurry. It’s like trying to draw a detailed picture with just a few pixels, we will miss the fine structure.\n\n**Very high dimension (e.g., 1000):**\n\n* With high dimensions, the model has plenty of space to learn detailed relationships between words like subtle differences in meaning or context. This can help on harder tasks if you have enough data.\n\n* If our dataset is large and diverse, high-dimensional embeddings can boost accuracy and help the model understand complex patterns more deeply.\n\n* Too many dimensions can make the model memorize rare words instead of learning general patterns. It also takes longer to train and needs more memory.\n\n* High-dimensional spaces can become sparse and harder to optimize. The model might struggle to find stable patterns, especially if the data isn’t rich enough.\n\n**Trade-offs:**\n\n* Representation power vs. overfitting: Higher dimensions let the model learn richer word meanings, but they need lots of data. If the data isn’t enough, the model might just memorize instead of generalize, this is called overfitting.\n\n* Training stability: Smaller embeddings are easier to train and more stable, but they might miss important patterns. The model could end up oversimplifying word meanings.\n\n* Downstream performance: To solve things like analogies (“king” is to “queen” as “man” is to “woman”), the model requires just enough dimensions to capture those relationships. When dimensions are too few, the patterns vanish; and when too many, random noise can get in the way.\n\n### **Q5. Computational Efficiency**\nWord2Vec with softmax requires computing a normalization term over the entire vocabulary, which becomes computationally expensive for large $V$.  \nAlternative training objectives such as **Negative Sampling** and **Hierarchical Softmax** are used to improve scalability.\n\n- Explain why **negative sampling** makes training feasible for very large vocabularies.\n- How does **hierarchical softmax**  reduce the complexity?\n- Compare the computational complexity of the following methods:\n\n  - **Full Softmax**:\n  - **Negative Sampling**:\n  - **Hierarchical Softmax**:\n\n  Use Big-O notation\n\n**A.**\n\n- Instead of computing a softmax over the entire vocabulary (which is expensive for large V), NS updates the embeddings of the target word and a small number K of negative samples. This reduces computation from **O(V⋅D)** per training example to **O(K⋅D)**, where **K≪V**. It works under the assumption that sampling “incorrect” words” randomly suffices to push embeddings apart, and that precise probability normalization is not strictly required.\nThis makes training feasible for very large vocabularies without losing most semantic quality.\n\n- HS replaces the flat softmax with a binary tree over the vocabulary, typically a Huffman tree where frequent words are near the root. Predicting a word becomes a traversal from the root to the word’s leaf, requiring only \n**O(log2V)** computations per training example. Frequent words are updated faster, and rare words are updated along their unique path. This reduces the computation while still producing a properly normalized probability distribution, unlike NS.\n\n| Method               | Complexity per training example | Characteristics                                                                         |\n| -------------------- | ------------------------------- | ------------------------------------------------------------------------------- |\n| Full Softmax         | (O(V * D))                  | Updates all embeddings per example — very expensive for large (V).              |\n| Negative Sampling    | (O(K * D))                  | Only updates target + (K) negatives; K is typically 5–20. Fast, approximate.    |\n| Hierarchical Softmax | (O(D * log V))             | Traverses path from root to leaf in a binary tree. Exact probability, scalable. |\n\n\n### **Q6. t-SNE and UMAP Visualization Analysis**\nAfter training your Word2Vec models, you plotted the embeddings using t-SNE and UMAP.  \n- What insights can you derive from the spatial clustering of words?\n- How do t-SNE and UMAP differ in capturing local vs global structures?\n- Which plot gave more interpretable clusters and why?\n- Can you identify any meaningful patterns (e.g., syntactic/semantic groupings)?\n\n**A.**\n\n- The plots show that model was learned successfully therefore we get defined clusters that are meaninful. They have words with similar meanings close to each other in a high-dimensional space. When we project this space down to 2D, those \"semantic neighborhoods\" stay together, forming the tight groups of color we see on the plots.\n\n- The key difference is:\n\n* t-SNE does well on local neighborhoods. Its primary goal is to make sure that a word's closest neighbors are kept right next to it in the 2D plot. However, the distance between different clusters in a t-SNE plot is often meaningless and can be misleading. It prioritizes keeping similar words together, but doesn't care where it puts the different groups in plot.\n\n* UMAP tries to preserve the global structure. This means the distances between the different colored clusters are more meaningful. If a \"countries\" cluster and a \"cities\" cluster appear close together in a UMAP plot, it's likely because they are more related to each other than, say, a cluster of \"animals.\" UMAP gives you a better sense of the overall landscape of our vocabulary.\n\n- The t-SNE plots were consistently more interpretable, especially the one for Skip-gram + Negative Sampling.\n\nThe reason is that t-SNE gave a better \"big picture\" view like islands on world map. It produced clusters that were both tight and well-separated. The clusters felt isolated, with no clear reason for why one was placed near another. \n\nUMAP plots appear more \"stringy\" and less defined. UMAP tries to balance the local structure with the global picture, which can sometimes result in a map that is more topologically honest but less readable. \n \n- Yes, the plots are full of meaningful patterns. The most obvious ones are semantic groupings. Each color represents a semantic neighborhood. For instance, in the skipgram_ns_tsne.html plot, we can see a point in blue colour cluster as cavalry and see that all neighbous are like soldiers, marines, commandos, airmen, troops and army. Another distinct orange cluster contains verbs of motion, while a pink one contains helping wordslike however, therefore,somehow etc.\n\nThis shows that models learned concepts based on context. They figured out that words like \"spain,\" \"france,\" and \"italy\" are used in similar ways and therefore must have similar meanings. While syntactic patterns (like verb tenses [verbs of motion like leaping, fleeing, tumbling, dragging]) were also learned, these semantic (meaning-based) clusters  stand out most clearly in these visualizations.\n\n","metadata":{"id":"yHhwEaktSLYM"}},{"cell_type":"markdown","source":"# **Task 2 : Naive Bayes**","metadata":{"id":"W8l-q_7eRZhx"}},{"cell_type":"markdown","source":"\n## **1. Concepts**  \nNaive Bayes is a **probabilistic machine learning algorithm** based on **Bayes’ theorem** with the simplifying assumption that all features are conditionally independent given the class. Despite its simplicity, it is widely used in domains such as spam detection, sentiment analysis, and text classification due to its efficiency and effectiveness on high-dimensional data.  \n\n---\n\n## **1.1 Motivation**  \n\n- Many classification tasks involve **large feature spaces** (e.g., words in documents).  \n- Complex models struggle with high dimensionality and require significant computation.  \n- Naive Bayes offers a **lightweight and interpretable** solution that is both **fast to train and test**.  \n- Works surprisingly well even when the independence assumption does not hold perfectly.  \n\n**Example:** Predict whether an email is *spam* or *ham* using word occurrences.  \n\n---\n\n## **1.2 Core Ideas**  \n\n## Naive Bayes Theory\n\n- **Bayes’ Theorem**\n\n$$\nP(C \\mid X) = \\frac{P(X \\mid C) \\, P(C)}{P(X)}\n$$\n\nWhere:  \n- \\($C$\\): Class label  \n- \\($X$\\): Feature vector (e.g., tokens in a document)  \n\n---\n\n### **1. Naive Independence Assumption**\n\n$$\nP(X \\mid C) = \\prod_{i=1}^n P(x_i \\mid C)\n$$\n\n\n- \\($X = (x_1, x_2, \\dots, x_n)$\\): the feature vector (all attributes/words in a document).  \n- \\($C$\\): the class label (e.g., spam/ham, positive/negative, topic).  \n- \\($x_i$\\): the \\($i$\\)-th feature of \\($X$\\) (e.g., presence/absence or count of word \\($i$\\)).  \n- \\($n$\\): the number of features (e.g., vocabulary size).  \n- \\($P(X \\mid C)$\\): probability of observing the full feature vector \\($X$\\) given class \\($C$\\).  \n- \\($P(x_i \\mid C)$\\): probability of observing the $i$-th feature given class \\($C$\\).  \n\n👉 The **naive** assumption is that features are conditionally independent given the class, so we can multiply their probabilities.\n\n---\n\n### **2. Classification Rule**\n\n$$\n\\hat{C} = \\arg\\max_C \\; P(C) \\prod_{i=1}^n P(x_i \\mid C)\n$$\n\n- \\($\\hat{C}$\\): predicted class for the instance (the classifier’s output).  \n- \\($\\arg\\max_C$\\): choose the class \\($C$\\) that maximizes the expression.  \n- \\($P(C)$\\): prior probability of class \\($C$\\) (from class frequencies).  \n- \\($\\prod_{i=1}^n P(x_i \\mid C)$\\): likelihood of observing \\($X$\\) under class \\($C$\\).  \n\n👉 The classifier picks the class that makes the features most probable under Bayes’ rule.\n\n\n- **Intuition**  \nNaive Bayes simplifies classification by assuming features are conditionally independent given the class. Instead of estimating a complex joint probability distribution, it multiplies individual feature likelihoods, making it efficient and effective for high-dimensional data like text.\n\n\n---\n\n## **1.4 Implementation Details**  \n\n## Naive Bayes Steps\n\n1. **Training Phase**  \n   - Compute **prior probabilities** $P(C)$ from class frequencies.  \n   - Estimate **conditional probabilities** $P(x_i \\mid C)$ from feature counts.  \n\n---\n\n2. **Smoothing**  \n- Use **Laplace smoothing** to avoid zero probabilities:  \n\n   $$\n   P(x_i \\mid C) = \\frac{\\text{count}(x_i, C) + 1}{\\sum_j \\text{count}(x_j, C) + |V|}\n   $$  \nwhere,\n-  $|V|$ = vocabulary size.  \n- \\($x_i$\\): the \\($i$\\)-th feature of \\($X$\\) (e.g., presence/absence or count of word \\($i$\\)).  \n\n---\n\n3. **Prediction Phase**  \n- Compute posterior probabilities for each class.  \n- Use logarithms to prevent underflow in large feature spaces.  \n- Select the class with maximum posterior probability.  \n\n\n## **1.5 Summary**  \n\n- Naive Bayes is a **probabilistic classifier** grounded in Bayes’ theorem.  \n- The “naive” assumption simplifies computation, making it **fast and scalable**.  \n- Variants handle different data types: multinomial, Bernoulli, and Gaussian.  \n- Despite its simplicity, it is one of the most **robust baseline algorithms** in machine learning, especially in **text classification**.  \n","metadata":{"id":"4kk2iJeJq4kY"}},{"cell_type":"markdown","source":"## **2. Task Explanation [Implementation - $200$ marks]**\n\n**Goal**:x\n\n- Implement the **Naive Bayes classifier** from scratch in Python (no external ML libraries allowed; only use Python standard libraries, numpy and libraries for tokenization like CountVectorizer, TfidfVectorizer from Scikit learn or any other tokenizer as you deem fit).\n\n- Use the below code to download the dataset for training and testing.\n```python\n# Expectation-Maximization\nnaive_bayes_train = load_dataset(\"Exploration-Lab/CS779-Fall25\", \"Assignment-3-naive-bayes\", split=\"train\")\nnaive_bayes_test = load_dataset(\"Exploration-Lab/CS779-Fall25\", \"Assignment-3-naive-bayes\", split=\"test\")\n```\n\n- Train your Naive Bayes model on the tokenized dataset and use it to predict class labels for the documents.    \n\n\n- Save the predicted labels in a file named `nb_predictions.csv`, ensuring one label per line for each input sample. As Naive Bayes is deterministic, your results should exactly match the output from the correct implementation of naive bayes.\n\n\n**Implementation Details**:\n\n1. **Preprocessing**  \n   - Perform normalization (lowercasing, punctuation removal if required).  \n   - Tokenize the documents into words (you may use simple whitespace or regex-based tokenization).  \n\n2. **Training Phase**  \n   - Compute **prior probabilities** of each class.  \n   - Compute **likelihood probabilities** for each word given each class using **Laplace smoothing**:  \n\n   $$\n   P(w \\mid C) = \\frac{\\text{count}(w, C) + 1}{\\sum_j \\text{count}(w_j, C) + |V|}\n   $$  \n\n   where \\( |V| \\) is the vocabulary size.  \n\n3. **Prediction Phase**  \n   - For each test document, compute the posterior probability:  \n\n   $$\n   P(C \\mid d) \\propto P(C) \\prod_{w \\in d} P(w \\mid C)\n   $$  \n\n   - Assign the label corresponding to the maximum posterior.  \n\n4. **Evaluation**  \n   - Compute **Accuracy, Precision, Recall, and F1-score** on the test set (with ground-truth labels).  \n   - Save results in `nb_results.txt`.  \n---\n**Note**: This assignment is a way to explore various trajectories for a given problem. Clarifying every single minute detail about the implementation like hyperparameters, tolerance limit for early stopping etc. will not be entertained on Discord. You can always explore multiple paths and select the most suitable solution for the assignment. You can make assumptions about the implementation details and document it in the code. It will be highly rewarded.\n\n---\n\n**Deliverables**:  \n- A file named `nb_predictions.csv` containing predicted labels.  \n- A file named `nb_results.txt` containing evaluation metrics.  \n\n---\n\n**Operating Constraints**:\n- DO NOT import any library except Python’s standard DO NOT import any library except Python standard library, numpy and for tokenization.\n- DO NOT use any ready-made Naive Bayes implementation (e.g., from scikit-learn).  \n\n---\n---\n\n**Proceed with clear, readable, and well-commented code!**\n","metadata":{"id":"aUUiak7Av1QW"}},{"cell_type":"code","source":"!pip install --upgrade datasets\nfrom datasets import load_dataset\nhf_token = \"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\"\nREPO_ID = \"Exploration-Lab/CS779-Fall25\" # this is the repository ID where the assignment data is stored","metadata":{"id":"IWvan0tga05f","outputId":"7a04899e-62df-44a3-e0cb-410be6942eb0"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Expectation-Maximization\nnaive_bayes_train = load_dataset(\"Exploration-Lab/CS779-Fall25\", \"Assignment-3-naive-bayes\", split=\"train\",token=hf_token)\nnaive_bayes_test = load_dataset(\"Exploration-Lab/CS779-Fall25\", \"Assignment-3-naive-bayes\", split=\"test\",token=hf_token)","metadata":{"id":"L_8tD4rBa03V","outputId":"a3906282-5306-41fc-8f17-bfa5627791be","trusted":true,"execution":{"iopub.status.busy":"2025-09-30T16:22:31.886142Z","iopub.execute_input":"2025-09-30T16:22:31.886413Z","iopub.status.idle":"2025-09-30T16:22:41.316112Z","shell.execute_reply.started":"2025-09-30T16:22:31.886395Z","shell.execute_reply":"2025-09-30T16:22:41.315151Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Assignment-3/naive_bayes/train_nb.parque(…):   0%|          | 0.00/190M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b004c6771a6f4512b9310c8f8f4e3438"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Assignment-3/naive_bayes/test_nb_with_la(…):   0%|          | 0.00/47.7M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f8e9dfeab8164a0197c90a7caae3e898"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/8000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e65effcb8b574204826c3a20ff65f0d0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating test split:   0%|          | 0/2000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"910075ccd4ff4245948a8408733152d5"}},"metadata":{}}],"execution_count":26},{"cell_type":"code","source":"import math\nimport random\nimport re\nimport numpy as np\nimport nltk\nfrom nltk.corpus import stopwords, wordnet\nfrom nltk.stem import WordNetLemmatizer\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom collections import Counter\nnltk.download('stopwords')\nnltk.download('wordnet')\nnltk.download('omw-1.4')\nSEED = 42\nnp.random.seed(SEED)\nrandom.seed(SEED)\nSTOPWORDS = set(stopwords.words('english'))\nLEMMATIZER = WordNetLemmatizer()\n\ndef simple_preprocess(text):\n    if not text:\n        return \"\"\n    text = text.lower()\n    text = re.sub(r\"[^a-z0-9\\s]\", \" \", text)  # Keep only letters, numbers, and spaces. This is a simple but effective\n    # way to remove punctuation and special symbols.\n    tokens = text.split()\n    tokens = [LEMMATIZER.lemmatize(t) for t in tokens if t not in STOPWORDS]\n    return \" \".join(tokens)\n\ndef extract(ds):\n    texts, labels = [], []\n    for item in ds:\n        text = getattr(item, \"text\", None) if hasattr(item, \"text\") else item.get(\"text\", None) if isinstance(item, dict) else None\n        label = getattr(item, \"category\", None) if hasattr(item, \"category\") else item.get(\"category\", None) if isinstance(item, dict) else None\n        if text and str(text).strip() and label is not None:\n            texts.append(str(text).strip())\n            labels.append(str(label).strip())\n    return texts, labels\n\nclass FastMultinomialNB:\n    def __init__(self, alpha=0.2):\n        self.alpha = float(alpha)\n        self.cv = None\n        self.vocab_size = None\n        self.classes_ = None\n        self.class_log_prior_ = None\n        self.feature_log_prob_ = None\n\n    def fit(self, raw_texts, raw_labels):\n        texts = [simple_preprocess(t) for t in raw_texts]\n        labels = [str(l) for l in raw_labels]\n\n        self.cv = CountVectorizer(token_pattern=r\"(?u)\\b\\w+\\b\")\n        X_train = self.cv.fit_transform(texts)\n        V = X_train.shape[1]\n        self.vocab_size = V\n\n        classes = sorted(set(labels))\n        self.classes_ = classes\n        n_classes = len(classes)\n\n        # Class priors\n        counts_by_class = Counter(labels)\n        total_docs = float(len(labels))\n        self.class_log_prior_ = np.array([math.log(counts_by_class[c] / total_docs) for c in classes], dtype=np.float64)\n        # We use log probabilities to prevent numerical underflow with very small\n        # Word counts per class\n        feature_count_matrix = np.zeros((n_classes, V), dtype=np.float64)\n        class_word_totals = np.zeros(n_classes, dtype=np.float64)\n        label_array = np.array(labels)\n          # I'm iterating through each class to sum up the word counts for all\n        # documents belonging to that class. This is more memory-efficient than\n        # creating dense sub-matrices.\n        for ci, c in enumerate(classes):\n            idx = np.nonzero(label_array == c)[0]\n            if idx.size == 0:\n                continue\n            Xc = X_train[idx, :]\n            wc = np.array(Xc.sum(axis=0)).ravel().astype(np.float64)\n            feature_count_matrix[ci, :] = wc\n            class_word_totals[ci] = wc.sum()\n\n         # Additive (Laplace) smoothing is crucial. It handles words\n        # that were not seen in the training data for a particular class.\n        # Without it, we'd get log(0), which is -infinity\n        denom = class_word_totals + self.alpha * V\n        denom = np.where(denom <= 0, self.alpha * V, denom)\n        self.feature_log_prob_ = np.log((feature_count_matrix + self.alpha) / denom[:, None])\n\n    def predict(self, raw_texts):\n        texts =[simple_preprocess(t) for t in raw_texts]\n        X_test= self.cv.transform(texts)\n        scores = X_test.dot(self.feature_log_prob_.T)+self.class_log_prior_[None, :]\n        pred_indices =np.asarray(scores.argmax(axis=1)).ravel()\n        return [self.classes_[int(i)] for i in pred_indices]\n\ndef run(naive_bayes_train, naive_bayes_test,\n                           ngram_range=(1,2), min_df=2, max_df=0.95, max_features=20000,\n                           alpha=0.2, use_lemmatize=True):\n    # extract\n    X_train, y_train = extract(naive_bayes_train)\n    X_test,  y_test  = extract(naive_bayes_test)\n    X_train = [simple_preprocess(t) for t in X_train]\n    X_test  = [simple_preprocess(t) for t in X_test]\n    y_train = [str(l) for l in y_train]\n    y_test  = [str(l) for l in y_test]\n\n    # vectorizer parameters\n    vec_params = {\n        \"token_pattern\": r\"(?u)\\b\\w+\\b\",\n        \"ngram_range\": ngram_range,\n        \"min_df\": min_df,\n        \"max_df\": max_df\n    }\n    if max_features is not None:\n        vec_params[\"max_features\"] = max_features\n    model = FastMultinomialNB(alpha=alpha)\n    try:\n        # many versions accept vectorizer_params keyword\n        model.fit(X_train, y_train, vectorizer_params=vec_params)\n    except TypeError:\n        # fallback: create a CountVectorizer with same params and set model.cv before calling fit\n        vec = CountVectorizer(**vec_params)\n        model.cv = vec\n        model.fit(X_train, y_train)\n    preds = model.predict(X_test)\n    with open(\"nb_predictions.csv\", \"w\", encoding=\"utf8\") as f:\n        for p in preds:\n            f.write(str(p) + \"\\n\")\n    # compute metrics\n    cm, classes = confusion_matrix_and_classes(y_test, preds)\n    precision, recall, f1=per_class_metrics_from_cm(cm)\n    accuracy=float(np.diag(cm).sum())/float(cm.sum()) if cm.sum()>0 else 0.0\n    macro_precision = float(np.mean(precision))\n    macro_recall= float(np.mean(recall))\n    macro_f1= float(np.mean(f1))\n    with open(\"nb_results.txt\", \"w\", encoding=\"utf8\") as f:\n        f.write(f\"Config:\\n\")\n        f.write(f\"  ngram_range: {ngram_range}\\n\")\n        f.write(f\"  min_df: {min_df}\\n\")\n        f.write(f\"  max_df: {max_df}\\n\")\n        f.write(f\"  max_features: {max_features}\\n\")\n        f.write(f\"  alpha: {alpha}\\n\\n\")\n        f.write(f\"Deterministic seed: {SEED}\\n\")\n        f.write(f\"Vocabulary size: {model.vocab_size}\\n\")\n        f.write(f\"Accuracy: {accuracy:.6f}\\n\")\n        f.write(f\"Macro-Precision: {macro_precision:.6f}\\n\")\n        f.write(f\"Macro-Recall:    {macro_recall:.6f}\\n\")\n        f.write(f\"Macro-F1:        {macro_f1:.6f}\\n\\n\")\n        f.write(\"Per-class metrics:\\n\")\n        for i, c in enumerate(classes):\n            f.write(f\"Class: {c}\\n\")\n            f.write(f\"  Support:   {cm.sum(axis=1)[i]}\\n\")\n            f.write(f\"  Precision: {precision[i]:.6f}\\n\")\n            f.write(f\"  Recall:    {recall[i]:.6f}\\n\")\n            f.write(f\"  F1-score:  {f1[i]:.6f}\\n\\n\")\n    print(f\"Accuracy: {accuracy:.4f}  Macro-F1: {macro_f1:.4f}  Vocab: {model.vocab_size}\")\n\ndef confusion_matrix_and_classes(y_true, y_pred):\n    classes = sorted(set(y_true) | set(y_pred))\n    idx ={c: i for i, c in enumerate(classes)}\n    K =len(classes)\n    cm= np.zeros((K, K), dtype=np.int64)\n    for t, p in zip(y_true, y_pred):\n        cm[idx[t], idx[p]]+=1\n    return cm, classes\n\ndef per_class_metrics_from_cm(cm):\n    TP = np.diag(cm).astype(np.float64)\n    FP = cm.sum(axis=0).astype(np.float64)-TP\n    FN = cm.sum(axis=1).astype(np.float64)-TP # Using np.errstate to avoid runtime warnings for division by zero.\n    # np.where is a safe way to handle these cases, returning 0.0 instead of NaN.\n    with np.errstate(divide='ignore', invalid='ignore'):\n        precision= np.where((TP+FP) > 0, TP/(TP+FP), 0.0)\n        recall =np.where((TP+FN)>0, TP/(TP+FN), 0.0)\n        f1 = np.where((precision + recall)>0, 2*precision*recall/(precision+recall), 0.0)\n    return precision, recall, f1\n\nif __name__ == \"__main__\":\n    CONFIG = {\n        \"ngram_range\": (1,1),\n        \"min_df\": 5,\n        \"max_df\": 0.85,\n        \"max_features\": 50000,\n        \"alpha\": 0.2\n    }\n    run(naive_bayes_train, naive_bayes_test,\n                           ngram_range=CONFIG[\"ngram_range\"],\n                           min_df=CONFIG[\"min_df\"],\n                           max_df=CONFIG[\"max_df\"],\n                           max_features=CONFIG[\"max_features\"],\n                           alpha=CONFIG[\"alpha\"])\n    # all hyperparamters were selected by experimenting. I chose that gave best accuracy.","metadata":{"id":"sGUtmFKfa0wJ","outputId":"84f999c4-6fc2-4906-ccad-e9b5e3fb40a3","trusted":true,"execution":{"iopub.status.busy":"2025-09-30T16:29:50.289501Z","iopub.execute_input":"2025-09-30T16:29:50.289876Z","iopub.status.idle":"2025-09-30T16:37:02.605970Z","shell.execute_reply.started":"2025-09-30T16:29:50.289842Z","shell.execute_reply":"2025-09-30T16:37:02.605035Z"}},"outputs":[{"name":"stderr","text":"[nltk_data] Downloading package stopwords to /usr/share/nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n[nltk_data] Downloading package wordnet to /usr/share/nltk_data...\n[nltk_data]   Package wordnet is already up-to-date!\n[nltk_data] Downloading package omw-1.4 to /usr/share/nltk_data...\n[nltk_data]   Package omw-1.4 is already up-to-date!\n","output_type":"stream"},{"name":"stdout","text":"Accuracy: 0.6415  Macro-F1: 0.6409  Vocab: 822786\n","output_type":"stream"}],"execution_count":28},{"cell_type":"markdown","source":"References:\n\nhttps://medium.com/@rangavamsi5/na%C3%AFve-bayes-algorithm-implementation-from-scratch-in-python-7b2cc39268b9\n\nhttps://github.com/akash18tripathi/Multinomial-Naive-Bayes-from-Scratch/blob/main/Multinomial-Naive-Bayes-from-Scratch.ipynb\n","metadata":{"id":"xBc_KK1lpalg"}},{"cell_type":"markdown","source":"\n## **3. Analysis (Critical Thinking & Exploration) [$5 \\times 20 = 100$ marks]**\n\n**You must answer the following five questions after implementing Naive Bayes' Classifier (Complete these in this colab file):**\n\n\n\n**Note**:  Answers should reflect your critical thought and, where possible, relate to your classifier model's architecture, output or performance.","metadata":{"id":"xk271KR0zrl3"}},{"cell_type":"markdown","source":"Q.1 Why is Naive Bayes considered a generative model, and   how does this differ from a discriminative model?","metadata":{"id":"hlUTTnzN0AeF"}},{"cell_type":"markdown","source":"Naive Bayes is considered a generative model because it models the **joint probability distribution**\nP(X,C) by learning how the data (features) could be generated given a class (P(X|C)) and combines it with the prior P(C). Prediction is done via Bayes’ rule.\nThis differs from a discriminative model (like logistic regression), which directly finds the **conditional probability P(C∣X)** or learns a **decision boundary** without finding how the data is generated.\nSo, Naive Bayes sees data generation as probabilistic (with independence assumption), while discriminative models separate classes.","metadata":{}},{"cell_type":"markdown","source":"Q.2 What is the role of the conditional independence assumption in Naive Bayes, and why is it often called “naive”?","metadata":{"id":"kcoPpT9nVCiH"}},{"cell_type":"markdown","source":"The conditional independence assumption says that, given the class C, every feature is independent of every other feature. It turns the hard joint P(X1,…,Xn∣C) into a product of simple **one-dimensional likelihoods ∏iP(Xi∣Y)**. This keeps the model tiny (few parameters) and training/prediction becomes very fast — which is why naive bayes is a robust baseline for high-dimensional problems (e.g., text). The conditional independence assumption makes **computation efficient**.\n\nThe conditional independence assumption is usually false in real data because words are often correlated in text, so it’s overly simple — i.e. naive. ","metadata":{"id":"QNksD-0_VIrf"}},{"cell_type":"markdown","source":"Q3. Why is Laplace (add-one) smoothing important in Naive Bayes, and what would happen if we do not use it?","metadata":{"id":"exH5SmgRVZAZ"}},{"cell_type":"markdown","source":"Laplace (add-one) smoothing ensures that no feature has **zero probability** in a class. If we don't use it then if a feature appears in the test set but never in the training set of a class, the likelihood P(Xi|C) becomes 0, making the entire class probability (product of all P(Xi|C)) zero. It can **misclassify** any sample containing unseen words/features. So, Laplace (add-one) smoothing prevents the Naive Bayes model from completely discarding a class due to unseen features. This **generalizes** the model.","metadata":{"id":"-xA2wy0mVcbt"}},{"cell_type":"markdown","source":"Q4. Let's compare topic classification done here using Naive Bayes to the topic classfication task in Assignment-1.\n  - *Q4-a.* Comparing Naive Bayes to K-Nearest Neighbours, which method has more training and inference time complexity and why? (10 Marks)\n  - *Q4-b.* Why does Naive Bayes perform better/worse than KNN? (10 Marks)","metadata":{"id":"DA9SfGKpVdN1"}},{"cell_type":"markdown","source":"a. **Training & Inference Time Complexity:**\n\n**Naive Bayes:** Very fast to train (just we compute word probabilities per class), O(N×V) for N docs and V sized vocabulary; inference is also fast, O(V×C) per document. (C = # of classes)\n\n**KNN:** No training cost (just we store the data), but inference is slow, O(N×V) per query, because it computes distances to all training points.\n\n**Conclusion:** KNN has higher inference time complexity; Naive Bayes is faster for both training and prediction.\n\nb. **Performance Comparison:**\n\n**Naive Bayes:** It outperforms KNN in text tasks because it handles high-dimensional sparse features efficiently and uses probabilistic class modeling.\n\n**KNN:** It struggles with sparse, high-dimensional text due to the curse of dimensionality; distance metrics become less meaningful.\n\n**Conclusion:** KNN can be better if feature correlations are strong and dense representations (e.g., embeddings) are used.\n\n","metadata":{"id":"ivDKS2KOViSr"}},{"cell_type":"markdown","source":"Q5. Why is Naive Bayes often said to perform well in text classification problems, despite its simplistic assumptions?","metadata":{"id":"oJEhQRtEVkgO"}},{"cell_type":"markdown","source":"Naive Bayes works well for text classification because:\n\n* Words are high-dimensional and sparse, so exact feature correlations don't matter much.\n\n* The conditional independence assumption is “good enough” in practice, capturing dominant word-class associations.\n\n* Efficiently handles large vocabularies and small training data.\n\n* Probabilistic output naturally ranks class likelihoods, making it robust for tasks like spam detection, sentiment analysis, or topic categorization.\n\n","metadata":{"id":"A5FFYiHUVoMu"}},{"cell_type":"markdown","source":"# **Task 3 : Expectation Maximization**","metadata":{"id":"9iHsITCeWIIc"}},{"cell_type":"markdown","source":"## **1. Concepts**\n\nExpectation Maximization (EM) is a **probabilistic optimization algorithm** used to estimate parameters of statistical models when data has **latent (hidden) variables**. It alternates between assigning probabilities to hidden variables (E-step) and maximizing the likelihood of the parameters (M-step). EM is widely applied in clustering (e.g., Gaussian Mixture Models), missing data problems, and probabilistic inference.  \n\n---\n\n## 1.1 Motivation\n\n- Real-world datasets often contain **incomplete or hidden information** (e.g., cluster assignments are unknown).  \n- Direct maximization of the likelihood function becomes intractable due to hidden variables.  \n- EM provides an **iterative framework** to estimate parameters efficiently by breaking the problem into two simpler steps.  \n\nExample:  \nClustering points into multiple Gaussian distributions when class labels are unknown.  \n\n---\n\n## 1.2 Core Ideas\n\n### (A) Theoretical Explanation\n\n- **Likelihood with hidden variables**:  \n\n$$\nP(X \\mid \\theta) = \\sum_Z P(X, Z \\mid \\theta)\n$$  \n\nWhere:  \n- \\( $X$ \\): Observed data  \n- \\( $Z$ \\): Latent (hidden) variables  \n- \\( $\\theta$ \\): Model parameters  \n\n- **E-step (Expectation)**:  \n  Estimate the posterior distribution of hidden variables given current parameters:  \n\n$$\nQ(Z) = P(Z \\mid X, \\theta^{(t)})\n$$  \n\n- **M-step (Maximization)**:  \n  Update parameters by maximizing the expected log-likelihood:  \n\n$$\n\\theta^{(t+1)} = \\arg\\max_\\theta \\, \\mathbb{E}_{Q(Z)}[\\log P(X, Z \\mid \\theta)]\n$$  \n\n- **Intuition**:  \n  The E-step “fills in” missing/hidden data with probabilities, while the M-step re-estimates parameters as if the hidden data were observed. This alternation improves the likelihood iteratively.  \n\n---\n\n### (B) Example: Gaussian Mixture Model (GMM) Clustering  \n\nGiven data points without labels:  \n\n- **E-step**: Compute the probability that each data point belongs to each Gaussian component.  \n- **M-step**: Update means, covariances, and mixture weights using these probabilities.  \n- Repeat until convergence.  \n\nThis way, EM allows clustering without prior labels.  \n\n---\n\n## 1.3 Variants of EM  \n\n1. **Gaussian Mixture Models (GMM-EM)**  \n   - Clustering with Gaussian distributions.  \n\n2. **Hidden Markov Models (HMM-EM, i.e., Baum-Welch Algorithm)**  \n   - Sequence data with hidden states.  \n\n3. **Soft K-Means**  \n   - EM with isotropic Gaussians and uniform variance assumption.  \n\n---\n\n## 1.4 Implementation Details  \n\n1. **Initialization**  \n   - Start with random guesses for parameters (e.g., means and covariances in GMM).  \n\n2. **E-step**  \n   - Calculate the probability distribution of hidden variables given observed data.  \n\n3. **M-step**  \n   - Update parameter estimates by maximizing the expected log-likelihood.  \n\n4. **Convergence**  \n   - Stop when parameters stabilize or the log-likelihood improvement falls below a threshold.   \n\n---\n\n## 1.5 Summary  \n\n- Expectation Maximization is an **iterative optimization algorithm** for parameter estimation in models with hidden variables.  \n- Alternates between **E-step (inferring hidden variables)** and **M-step (optimizing parameters)**.  \n- Widely applied in **clustering, HMMs, and incomplete-data problems**.  \n- Sensitive to initialization but remains a cornerstone in unsupervised learning.  \n\n---\n\n","metadata":{"id":"tVDZz5ciWIId"}},{"cell_type":"markdown","source":"## **2. Task Explanation [Implementation - $200$ marks]**\n\n**Goal**:\n\n- Implement the **Expectation-Maximization (EM) Algorithm** from scratch in Python (no external ML libraries allowed; only use Python standard libraries, numpy and libraries for tokenization like CountVectorizer, TfidfVectorizer from Scikit learn or any other tokenizer as you deem fit).\n\n- Use the below code to download the dataset for training and testing.\n```python\n# Expectation-Maximization\nem_train = load_dataset(\"Exploration-Lab/CS779-Fall25\", \"Assignment-3-em\", split=\"train\")\nem_test = load_dataset(\"Exploration-Lab/CS779-Fall25\", \"Assignment-3-em\", split=\"test\")\n```\n\n- Use BOW.\n\n- Apply the EM algorithm to cluster the tokenized dataset into a predefined number of clusters (10). Treat each document as a bag of tokens.\n\n- Save the predicted cluster assignments to a file named `em_predictions.csv`, with one cluster number per line (corresponding to each input document).\n\n- **Adjusted Rand Index (ARI)** will be used to evaluate your predictions with respect to the true labels of test set. ARI is a clustering evaluation metric that considers all pairs of samples in the dataset and for each  each pair, it checks whether the samples are:\n  - In the same cluster in both true and predicted labels (agreement ✅)\n\n  - In different clusters in both true and predicted labels (agreement ✅)\n\n  - In the same cluster in one but different in the other (disagreement ❌)\n\n  - A score of 0 represents random clustering in the scale of [-1,+1]. A well imppelemted EM algorithm should yield positive ARI scores.\n\n---\n- **Note**: This assignment is a way to explore various trajectories for a given problem. Clarifying every single minute detail about the implementation like hyperparameters, tolerance limit for early stopping etc. will not be entertained on Discord. You can always explore multiple paths and select the most suitable solution for the assignment. You can make assumptions about the implementation details and document it in the code. It will be highly rewarded.\n\n---\n\n- **Deliverables**:  \n  - `em_predictions.csv`\n---\n\n- **Operating constraints**:  \n  - DO NOT import any library except Python standard library, numpy and for tokenization.  \n  - DO NOT use any ready-made EM algorithm or clustering implementation.\n\n---\n---\n\n**Proceed with clear, readable, and well-commented code!**","metadata":{"id":"qnSH2rNoWIId"}},{"cell_type":"code","source":"# Expectation-Maximization\nem_train = load_dataset(\"Exploration-Lab/CS779-Fall25\", \"Assignment-3-em\", split=\"train\",token=hf_token)\nem_test = load_dataset(\"Exploration-Lab/CS779-Fall25\", \"Assignment-3-em\", split=\"test\",token=hf_token)\n","metadata":{"id":"ixjutKauW50Y","outputId":"928c312b-7d24-4dec-d494-f7fb41ff29b1","trusted":true,"execution":{"iopub.status.busy":"2025-09-30T16:05:02.492284Z","iopub.execute_input":"2025-09-30T16:05:02.492599Z","iopub.status.idle":"2025-09-30T16:05:10.722363Z","shell.execute_reply.started":"2025-09-30T16:05:02.492571Z","shell.execute_reply":"2025-09-30T16:05:10.721491Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Assignment-3/em/train_em.parquet:   0%|          | 0.00/189M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"df13b57ed93f4779ae116b88d32de376"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Assignment-3/em/test_em.parquet:   0%|          | 0.00/47.7M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b95fd73b450741ac876a0ac37d0607bb"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/8000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"522e9fbe746d42ca8e863b4f6a57cee1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating test split:   0%|          | 0/2000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2b79f7a88594426cba1b450ee39297bf"}},"metadata":{}}],"execution_count":23},{"cell_type":"code","source":"import time\nimport math\nimport random\nfrom collections import Counter\nimport numpy as np\nfrom sklearn.feature_extraction.text import CountVectorizer\nSEED = 42\nrandom.seed(SEED)\nnp.random.seed(SEED)\n# import spacy\nimport nltk\nimport re\n\n# # Load the spaCy model once at the start of your script\n# # You may need to run this command in your terminal first:\n# # python -m spacy download en_core_web_sm\n# try:\n#     NLP = spacy.load(\"en_core_web_sm\")\n# except OSError:\n#     print(\"Downloading spaCy model 'en_core_web_sm'...\")\n#     from spacy.cli import download\n#     download(\"en_core_web_sm\")\n#     NLP = spacy.load(\"en_core_web_sm\")\n# import nltk\nnltk.download('stopwords', quiet=True)\nnltk.download('wordnet', quiet=True)\nnltk.download('omw-1.4', quiet=True)\nfrom nltk.corpus import stopwords\nfrom nltk.stem import WordNetLemmatizer\nSTOPWORDS = set(stopwords.words('english'))\nLEMMATIZER = WordNetLemmatizer() # or topic/clustering tasks we want a compact vocabulary\n    # that preserves meaning (lemmatization) while removing noise (stopwords).\n    # I used nltk because spacy is heavy and takes time\n\n# def simple_preprocess(text):\n\n#     doc = NLP(text.lower(), disable=['parser', 'ner','pos'])\n\n#     # Create a list of lemmatized tokens\n#     lemmas = []\n#     for token in doc:\n#         # 1. Check if it's a stop word or punctuation\n#         if token.is_stop or token.is_punct:\n#             continue\n#         # 2. Check if it contains only alphabetic characters\n#         if not token.is_alpha:\n#             continue\n\n#         lemmas.append(token.lemma_)\n\n#     # Join the lemmas back into a single string\n#     return \" \".join(lemmas)\n\n\n\ndef simple_preprocess(text):\n    if text is None:\n        return \"\"\n    s= str(text).lower()\n    s =re.sub(r\"[^a-z0-9\\s]\", \" \", s)         # keep letters/numbers & spaces\n    s =re.sub(r\"\\s+\", \" \", s).strip()\n    if not s:\n        return \"\"\n    tokens = s.split()\n    # remove stopwords and lemmatize\n    proc=[]\n    for t in tokens:\n        if t in STOPWORDS:\n            continue\n        # basic lemmatize (WordNet lemmatizer defaults to noun. It is lightweight)\n        lt=LEMMATIZER.lemmatize(t)\n        proc.append(lt)\n    return \" \".join(proc)\n\ndef extract(ds): # To Extract text and labels from dataset \n    texts= []\n    labels =[]\n    for item in ds:\n        # text\n        if isinstance(item, dict):\n          text =item.get(\"text\", None)\n          label =item.get(\"category\", None)\n        else:\n          text =getattr(item, \"text\", None) if hasattr(item, \"text\") else None\n          label =getattr(item, \"category\", None) if hasattr(item, \"category\") else None\n        texts.append(\"\" if text is None else str(text))\n        labels.append(label)\n    return texts, labels\n\ndef row_logsumexp(mat): # Row-wise log-sum-exp trick to avoid numerical instability ( I did not use this earlier but was getting NaN values so had to implement this way)\n    # mat: numpy array   \n    m =np.max(mat, axis=1)\n    m_safe=m.copy()   ## - I computed responsibilities in log-space and use a row-wise log-sum-exp trick\n#   to avoid underflow when multiplying many small probabilities.\n    m_safe[np.isneginf(m_safe)]=0.0\n    s = np.log(np.sum(np.exp(mat - m_safe[:, None]), axis=1))+m_safe\n    s[np.isneginf(m)] = -np.inf\n    return s\n\nclass MultinomialMixtureEM:\n    def __init__(self, n_components=10, alpha=0.1, max_iter=80, tol=1e-4, verbose=False):\n        self.K =int(n_components)\n        self.alpha= float(alpha)\n        self.max_iter= int(max_iter)\n        self.tol =float(tol)\n        self.verbose = verbose\n        self.pi =None          # mixture weights (K,)\n        self.log_phi =None      # mixture weights (K,)\n        self.vocab_size = 0\n        self.converged_ = False\n\n    def _init_params(self, X):\n        # To  Initialize mixture weights and component distributions\n        N, V = X.shape\n        self.vocab_size = V\n        self.pi = np.full(self.K, 1.0/self.K, dtype=np.float64)\n        rng = np.random.RandomState(SEED) # phi is component_word_dist\n        phi = rng.gamma(1.0, 1.0, size=(self.K, V)).astype(np.float64)\n        phi = (phi+self.alpha)\n        phi = phi/phi.sum(axis=1)[:, None]\n        self.log_phi = np.log(phi)\n\n    def fit(self, X):\n        # X is expected to be scipy.sparse.csr_matrix (CountVectorizer output)\n        N, V = X.shape\n        self._init_params(X)\n        ll_hist = []\n        Xcsr = X.tocsr()\n        for it in range(self.max_iter):\n            t0 = time.time()\n            # E-step: compute log responsibilities: log r_nk ∝ log pi_k + X_n · log_phi_k\n            X_dot_logphi =Xcsr.dot(self.log_phi.T)              # (N, K) sparse dot dense\n            log_pi = np.log(self.pi + 1e-20)                    # (K,)\n            log_r_unnorm= X_dot_logphi+log_pi[None, :]       # (N, K)\n            row_lse =row_logsumexp(log_r_unnorm)               # (N,)\n            total_loglike = np.sum(row_lse)\n            ll_hist.append(total_loglike)\n            print(f\"[EM] iter {it:2d} loglike {total_loglike:.6f} time {time.time()-t0:.2f}s\")\n            # responsibilities r_nk\n            r =np.exp(log_r_unnorm-row_lse[:, None])        # (N, K) dense but K small\n            # M-step: update pi\n            nk = r.sum(axis=0)                                 # (K,)\n             # nk is component_mass\n            pi_new =nk/float(N)\n            # update phi: weighted counts per component\n            # For each k: counts_k = sum_n r_nk * X_n (vector length V)\n            feature_count_matrix=np.zeros((self.K, V), dtype=np.float64)\n            # for k in range(self.K):\n            #     weights = r[:, k]\n            #     # X.multiply(weights[:,None]) scales rows by weights  # I vectorized this below\n            #     Xw = Xcsr.multiply(weights[:, None])\n            #     wk = np.array(Xw.sum(axis=0)).ravel().astype(np.float64)\n            #     feature_count_matrix[k, :] = wk\n            feature_count_matrix = r.T @ Xcsr\n            # Added Dirichlet smoothing (alpha) to avoid zeros\n            numer =feature_count_matrix + self.alpha\n            denom= numer.sum(axis=1)\n            denom= denom +1e-20\n            phi_new = numer/denom[:, None]\n            # store\n            self.pi =pi_new\n            self.log_phi =np.log(phi_new)\n            # check convergence\n            if it > 0:\n                delta = ll_hist[-1]-ll_hist[-2] ## log-likelihood often plateaus slowly; tolerance prevents wasted iterations.\n                if abs(delta)<self.tol:\n                    self.converged_ = True\n                    break\n        return ll_hist\n\n    def predict(self, X):\n        Xcsr =X.tocsr() # - Sparse-dense operations (CSR @ dense) are used to keep memory and CPU efficient\n        scores=Xcsr.dot(self.log_phi.T)+np.log(self.pi + 1e-20)[None, :]  # (N,K)\n        preds=np.asarray(np.argmax(scores, axis=1)).ravel()\n        return preds\ndef run_em(em_train, em_test,\n                              n_components=10,\n                              alpha=0.1,\n                              max_iter=80,\n                              tol=1e-4,\n                              min_df=2,\n                              max_df=0.95,\n                              max_features=20000,\n                              save_preds=\"em_predictions.csv\"\n                              ):\n    Xtr_raw, ytr = extract(em_train)\n    Xte_raw, yte = extract(em_test)\n    Xtr = [simple_preprocess(t) for t in Xtr_raw]\n    Xte = [simple_preprocess(t) for t in Xte_raw]\n\n    # vectorizer: unigrams with bigrams (bigrams worked better on news20 test dataset giving better ARI. Hopefully this would be the case in our dataset too)\n    vec = CountVectorizer(ngram_range=(1,2), token_pattern=r\"(?u)\\b\\w+\\b\",\n                          min_df=min_df, max_df=max_df, max_features=max_features)\n    X_train_counts = vec.fit_transform(Xtr)\n    X_test_counts = vec.transform(Xte)\n    # EM\n    model = MultinomialMixtureEM(n_components=n_components, alpha=alpha, max_iter=max_iter, tol=tol)\n    t0 = time.time()\n    ll_hist = model.fit(X_train_counts)\n    preds = model.predict(X_test_counts).astype(int)\n    with open(save_preds, \"w\", encoding=\"utf8\") as f:\n        for p in preds:\n            f.write(str(int(p)) + \"\\n\")  ##I set alpha via experimenting with news20 dataset, balancing robustness vs. over-smoothing.\nrun_em(em_train, em_test,n_components=10,alpha=0.1,\n                                                       max_iter=200,\n                                                       tol=1e-4,\n                                                       min_df=5,\n                                                       max_df=0.90,\n                                                       max_features=10000,\n                                                       save_preds=\"em_predictions.csv\")\n#Note: All hyperparamteres like min_df, max_df,max_features and ngram was set by testing with news20 dataset. This will not always be correct since dataset is different but I did not follow it blindly and kept a bit easy on which hyperparameters gave best result\n","metadata":{"id":"EsTYe_4oa2Y5","outputId":"bfc46f5c-a04d-4221-bee8-de993a425115","trusted":true,"execution":{"iopub.status.busy":"2025-09-30T16:05:10.723773Z","iopub.execute_input":"2025-09-30T16:05:10.724018Z","iopub.status.idle":"2025-09-30T16:12:24.260166Z","shell.execute_reply.started":"2025-09-30T16:05:10.723998Z","shell.execute_reply":"2025-09-30T16:12:24.258960Z"}},"outputs":[{"name":"stdout","text":"[EM] iter  0 loglike -288483043.473652 time 0.15s\n[EM] iter  1 loglike -250024178.713751 time 0.11s\n[EM] iter  2 loglike -246642076.966823 time 0.10s\n[EM] iter  3 loglike -245367229.628275 time 0.10s\n[EM] iter  4 loglike -244757166.341215 time 0.11s\n[EM] iter  5 loglike -244503123.359349 time 0.10s\n[EM] iter  6 loglike -244388882.640015 time 0.11s\n[EM] iter  7 loglike -244297346.227610 time 0.11s\n[EM] iter  8 loglike -244243613.955366 time 0.11s\n[EM] iter  9 loglike -244189212.581862 time 0.11s\n[EM] iter 10 loglike -244137986.009766 time 0.11s\n[EM] iter 11 loglike -244120208.163988 time 0.11s\n[EM] iter 12 loglike -244108001.308168 time 0.11s\n[EM] iter 13 loglike -244095390.194485 time 0.10s\n[EM] iter 14 loglike -244092683.874509 time 0.11s\n[EM] iter 15 loglike -244091334.343678 time 0.11s\n[EM] iter 16 loglike -244090253.790920 time 0.11s\n[EM] iter 17 loglike -244089748.241227 time 0.11s\n[EM] iter 18 loglike -244089395.172218 time 0.11s\n[EM] iter 19 loglike -244089013.245784 time 0.11s\n[EM] iter 20 loglike -244088706.161390 time 0.11s\n[EM] iter 21 loglike -244088509.647153 time 0.11s\n[EM] iter 22 loglike -244088420.368995 time 0.11s\n[EM] iter 23 loglike -244088420.356577 time 0.11s\n[EM] iter 24 loglike -244088419.776261 time 0.11s\n[EM] iter 25 loglike -244088362.288496 time 0.11s\n[EM] iter 26 loglike -244088337.661254 time 0.11s\n[EM] iter 27 loglike -244088337.661430 time 0.11s\n[EM] iter 28 loglike -244088337.661447 time 0.11s\n","output_type":"stream"}],"execution_count":24},{"cell_type":"markdown","source":"References:\n\nhttps://towardsdatascience.com/implementing-expectation-maximisation-algorithm-from-scratch-with-python-9ccb2c8521b3/","metadata":{}},{"cell_type":"markdown","source":"\n## **3. Analysis (Critical Thinking & Exploration) [$5 \\times 20 = 100$ marks]**\n\n**You must answer the following five questions after implementing Expectation Maximization algorithm (Complete these in this colab file):**\n\n\n\n**Note**:  Answers should reflect your critical thought and, where possible, relate to your algorithm and its performance.","metadata":{"id":"K53TcpWYWIIe"}},{"cell_type":"markdown","source":"Q.1 Why is Expectation Maximization considered an iterative optimization algorithm, and how does it differ from direct likelihood maximization?","metadata":{"id":"PetY89jBWIIe"}},{"cell_type":"markdown","source":"Expectation Maximization (EM) is considered iterative because it **alternates** between estimating hidden variables (E-step) and updating model parameters (M-step) repeatedly until convergence. Instead of directly solving for the parameters that maximize the likelihood which is impossible when latent variables exist. EM breaks the problem into manageable steps: first “guess” the missing data distribution, then optimize parameters based on that guess. This differs from direct likelihood maximization, which would try to solve a single global optimization problem.\n\nIt’s like iteratively refining both the “assignment of points to clusters” and “cluster definitions” rather than trying to find both at once.","metadata":{"id":"HdJVt0B5WIIe"}},{"cell_type":"markdown","source":"Q.2 What is the role of the E-step and M-step in EM, and why are they repeated until convergence?  ","metadata":{"id":"uxnnCJQHWIIe"}},{"cell_type":"markdown","source":"In EM, the E-step estimates the hidden  variables (e.g., cluster memberships) given the current model parameters, “guessing” how each data point fits into each component. The M-step then updates the model parameters to maximize the likelihood, treating the E-step assignments as soft counts. This alternating process is repeated until convergence because each step depends on the other: better parameter estimates improve the latent assignments, and better assignments improve parameter estimates. Iterating ensures the model gradually reaches a stable likelihood peak, even without labels.\n\nIn practical terms for a classifier, the E-step refines which cluster each document likely belongs to, and the M-step adjusts the word distributions and mixture weights, gradually improving the model’s fit to the text data.","metadata":{"id":"yElYpvy1WIIe"}},{"cell_type":"markdown","source":"Q.3 Why is EM sensitive to initialization, and how can this problem be mitigated in practice?  ","metadata":{"id":"EU-nc6uBWIIe"}},{"cell_type":"markdown","source":"EM is sensitive to initialization because it optimizes a **non-convex likelihood function**; starting with poor initial parameters can lead to convergence at **suboptimal** local maxima of the likelihood.\n\nMitigation strategies:\n\n* Run EM several times and pick the model with the highest final likelihood.\n\n* Use K-Means or domain knowledge to set starting cluster centers.\n\n* Smoothing parameters can prevent extreme probabilities and stabilize early iterations.","metadata":{"id":"d5LtGym-WIIe"}},{"cell_type":"markdown","source":"Q.4 What are some advantages and disadvantages of EM compared to other clustering/optimization algorithms (e.g., K-Means, gradient-based methods)?  ","metadata":{"id":"wGZkYkH7WIIe"}},{"cell_type":"markdown","source":"**Advantages:** \n\n* Unlike K-Means, which gives hard cluster assignments, EM provides soft probabilities, capturing uncertainty in cluster membership.\n\n* EM can model complex data distributions (e.g., Multinomial, Gaussian) rather than just Euclidean distances.\n\n* It maximizes the likelihood of the data under the chosen model, giving a statistical foundation.\n\n**Disadvantages:** \n\n* Poor starting points can lead to local optima, requiring multiple runs or careful seeding.\n\n* Each iteration is computationally heavier, especially for large vocabularies or datasets.\n\n* Performance depends on whether the chosen distribution matches the real data; mismatched assumptions can degrade results.\n\nEM is great if we want probabilistic clusters and can afford computation, but for large-scale, high-dimensional data, sometimes simpler methods like K-Means with feature engineering are more practical.","metadata":{"id":"wZzsq2Y4WIIg"}},{"cell_type":"markdown","source":"Q.5 Compare the results obtained in EM with results obtained from Naive bayes. Also delve upon how can we predict labels of a test dataset without labels after EM and why it can work?","metadata":{"id":"hJ2HrzP1WIIg"}},{"cell_type":"markdown","source":"Naive Bayes provides supervised, label-specific predictions, while EM performs unsupervised clustering without using labels. I cannot compare them as they are for differet datasets. I don't have true labels for EM dataset. \n\nAfter EM converges to give predictions, the parameters of the underlying distribution (e.g., cluster means, covariances, and mixing weights) are estimated. Even if the test set has no labels, we can assign each test point to the cluster where it has the highest posterior probability under these learned parameters. This works because EM learns the hidden structure in the data. Once that structure is captured, new points can be placed using it. The mapping between clusters and true labels is not always fully right, but the method gives a way to predict labels of a test dataset without labels after EM.","metadata":{"id":"lBif4fDeWIIg"}}]}